{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "together_dropout_0.1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/together_dropout_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "outputId": "d034ff51-0ba1-4235-9017-d9ed5237cd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/gdrive/My Drive/lungs/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_RXrcQgIkv3",
        "outputId": "146ccc80-5932-44a6-94da-799539b583ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "outputId": "5fd2a29b-2b2e-4ddf-e4e9-c93734f71ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "\n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "176it [00:00, 1047.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self,TAB,A, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "        self.TAB=TAB\n",
        "        self.A=A\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image,\n",
        "            'tab':self.TAB[self.tensor_files[item].stem],\n",
        "            'slope':self.A[self.tensor_files[item].stem]\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGULd4YJatQz"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 10\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "# cum = 0\n",
        "# for i in range(len(train)):\n",
        "#     sample = train[i]['image']\n",
        "#     cum += torch.mean(sample).item()\n",
        "\n",
        "# assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "\n",
        "        #tabular\n",
        "        self.tab1=nn.Linear(104,256)\n",
        "        self.tab2=nn.Linear(256,256)\n",
        "        self.tab3=nn.Linear(256,100)\n",
        "\n",
        "\n",
        "        #output\n",
        "        self.out1=nn.Linear(100,256)\n",
        "        self.out2=nn.Linear(256,256)\n",
        "        self.out3=nn.Linear(256,1)\n",
        "\n",
        "\n",
        "        #dropout\n",
        "        self.drop1=nn.Dropout(0.1)\n",
        "        self.drop2=nn.Dropout(0.1)\n",
        "        self.drop3=nn.Dropout(0.1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x,y, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "\n",
        "        x = torch.cat((x,y),1)\n",
        "        x = self.act(self.tab1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.act(self.tab2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.act(self.tab3(x))\n",
        "\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4,x\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x,y):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4,out = self.encode(x,y)\n",
        "        \n",
        "        out1=self.act(self.out1(out))\n",
        "        out1=self.drop3(out1)\n",
        "        out2=self.act(self.out2(out1))\n",
        "        output=self.out3(out2)\n",
        "\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var,output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB",
        "outputId": "98c11ec2-441f-40f4-a964-17021254a425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(300):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            tabular=inputs['tab'].to(device, dtype=torch.float)\n",
        "            slopes_input=inputs['slope'].to(device, dtype=torch.float)\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var,slope_output = model(inputs,tabular)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                slope_loss = F.mse_loss(slope_output,slopes_input)\n",
        "                \n",
        "                loss =  reconst_loss + kl_div+slope_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 train       :   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       :  89%|████████▉ | 8/9 [00:28<00:03,  3.48s/it, loss=997605.367188]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       : 100%|██████████| 9/9 [00:31<00:00,  3.51s/it, loss=961075.125887]\n",
            "Epoch 1 val         :  67%|██████▋   | 2/3 [00:01<00:00,  1.04it/s, loss=743916.000000]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 val         : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=691388.623661]\n",
            "Epoch 2 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=673208.105940]\n",
            "Epoch 2 val         : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=585424.618527]\n",
            "Epoch 3 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=606454.059176]\n",
            "Epoch 3 val         : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=546191.328571]\n",
            "Epoch 4 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=566800.978059]\n",
            "Epoch 4 val         : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=509715.880804]\n",
            "Epoch 5 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=522345.915559]\n",
            "Epoch 5 val         : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=467494.589732]\n",
            "Epoch 6 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=487254.159353]\n",
            "Epoch 6 val         : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=445716.226116]\n",
            "Epoch 7 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=452056.456117]\n",
            "Epoch 7 val         : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=403635.068080]\n",
            "Epoch 8 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=410049.165115]\n",
            "Epoch 8 val         : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=363974.258705]\n",
            "Epoch 9 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=371883.291667]\n",
            "Epoch 9 val         : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=335533.187835]\n",
            "Epoch 10 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=341495.350842]\n",
            "Epoch 10 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=306176.788170]\n",
            "Epoch 11 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=312327.064384]\n",
            "Epoch 11 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=279327.235156]\n",
            "Epoch 12 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=286179.258311]\n",
            "Epoch 12 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=256278.275000]\n",
            "Epoch 13 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=264845.344858]\n",
            "Epoch 13 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=237063.058929]\n",
            "Epoch 14 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=246549.075355]\n",
            "Epoch 14 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=226909.885156]\n",
            "Epoch 15 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=234796.790559]\n",
            "Epoch 15 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=213686.647098]\n",
            "Epoch 16 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=225119.412012]\n",
            "Epoch 16 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=205688.156250]\n",
            "Epoch 17 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=217202.152815]\n",
            "Epoch 17 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=201500.051339]\n",
            "Epoch 18 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=211922.056516]\n",
            "Epoch 18 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=194878.512277]\n",
            "Epoch 19 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=208704.512965]\n",
            "Epoch 19 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=194628.383705]\n",
            "Epoch 20 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=204912.361148]\n",
            "Epoch 20 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=188507.945759]\n",
            "Epoch 21 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=199611.954898]\n",
            "Epoch 21 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=184668.318304]\n",
            "Epoch 22 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=195390.357491]\n",
            "Epoch 22 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=181296.241183]\n",
            "Epoch 23 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=191163.312168]\n",
            "Epoch 23 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=177910.569643]\n",
            "Epoch 24 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=188001.412899]\n",
            "Epoch 24 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=174426.330859]\n",
            "Epoch 25 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=185884.357934]\n",
            "Epoch 25 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=172806.146150]\n",
            "Epoch 26 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=183573.903369]\n",
            "Epoch 26 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=170690.165290]\n",
            "Epoch 27 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=180803.023382]\n",
            "Epoch 27 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=168368.959208]\n",
            "Epoch 28 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=178299.940714]\n",
            "Epoch 28 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=166072.100112]\n",
            "Epoch 29 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=176450.400155]\n",
            "Epoch 29 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=163464.677902]\n",
            "Epoch 30 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=174011.210106]\n",
            "Epoch 30 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=162007.156250]\n",
            "Epoch 31 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=172645.972241]\n",
            "Epoch 31 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=160255.530246]\n",
            "Epoch 32 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=169915.775931]\n",
            "Epoch 32 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=158401.303460]\n",
            "Epoch 33 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=168310.325244]\n",
            "Epoch 33 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=156468.745926]\n",
            "Epoch 34 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=166517.290559]\n",
            "Epoch 34 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=154936.011105]\n",
            "Epoch 35 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=164755.477172]\n",
            "Epoch 35 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=153901.010268]\n",
            "Epoch 36 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=163847.421210]\n",
            "Epoch 36 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=152794.886886]\n",
            "Epoch 37 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=163314.272385]\n",
            "Epoch 37 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=151847.979967]\n",
            "Epoch 38 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=162567.191545]\n",
            "Epoch 38 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=150614.969308]\n",
            "Epoch 39 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=161100.050975]\n",
            "Epoch 39 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=149768.513170]\n",
            "Epoch 40 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=159867.984375]\n",
            "Epoch 40 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=149600.457533]\n",
            "Epoch 41 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=159625.532691]\n",
            "Epoch 41 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=147868.279911]\n",
            "Epoch 42 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=158210.819592]\n",
            "Epoch 42 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=147023.881696]\n",
            "Epoch 43 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=157415.859264]\n",
            "Epoch 43 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=147320.218917]\n",
            "Epoch 44 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=157491.021997]\n",
            "Epoch 44 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=146080.293471]\n",
            "Epoch 45 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=156075.671653]\n",
            "Epoch 45 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=144943.985212]\n",
            "Epoch 46 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=154957.314827]\n",
            "Epoch 46 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=144409.326116]\n",
            "Epoch 47 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=154400.043661]\n",
            "Epoch 47 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=143532.980859]\n",
            "Epoch 48 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=153771.165559]\n",
            "Epoch 48 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=143307.715067]\n",
            "Epoch 49 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=153605.121786]\n",
            "Epoch 49 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=142838.499498]\n",
            "Epoch 50 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=152949.951518]\n",
            "Epoch 50 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=142209.300614]\n",
            "Epoch 51 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=152792.783245]\n",
            "Epoch 51 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=141728.835156]\n",
            "Epoch 52 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=152019.380042]\n",
            "Epoch 52 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=141108.090681]\n",
            "Epoch 53 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=151089.206893]\n",
            "Epoch 53 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=140447.589397]\n",
            "Epoch 54 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=150363.938830]\n",
            "Epoch 54 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=139968.140904]\n",
            "Epoch 55 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=150132.667332]\n",
            "Epoch 55 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=139595.587221]\n",
            "Epoch 56 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=149505.541722]\n",
            "Epoch 56 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=139066.153571]\n",
            "Epoch 57 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=149086.710993]\n",
            "Epoch 57 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=138798.651786]\n",
            "Epoch 58 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=148640.828679]\n",
            "Epoch 58 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=138622.517467]\n",
            "Epoch 59 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=149058.750776]\n",
            "Epoch 59 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=138344.416127]\n",
            "Epoch 60 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=148354.145889]\n",
            "Epoch 60 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=137895.046429]\n",
            "Epoch 61 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=147446.070922]\n",
            "Epoch 61 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=137592.923717]\n",
            "Epoch 62 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=147498.329621]\n",
            "Epoch 62 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=137323.762500]\n",
            "Epoch 63 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=147078.083610]\n",
            "Epoch 63 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=137045.180078]\n",
            "Epoch 64 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=147019.197584]\n",
            "Epoch 64 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=136738.835603]\n",
            "Epoch 65 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=146644.084497]\n",
            "Epoch 65 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=136501.776507]\n",
            "Epoch 66 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=146545.828845]\n",
            "Epoch 66 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=136464.473940]\n",
            "Epoch 67 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=146331.397163]\n",
            "Epoch 67 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=136094.762165]\n",
            "Epoch 68 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=146131.382646]\n",
            "Epoch 68 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=136032.840402]\n",
            "Epoch 69 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=146066.831560]\n",
            "Epoch 69 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=135758.900056]\n",
            "Epoch 70 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145920.950244]\n",
            "Epoch 70 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=135629.866797]\n",
            "Epoch 71 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145176.640736]\n",
            "Epoch 71 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=135447.804688]\n",
            "Epoch 72 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145159.216423]\n",
            "Epoch 72 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=135329.759263]\n",
            "Epoch 73 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145368.915669]\n",
            "Epoch 73 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=135200.042857]\n",
            "Epoch 74 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144700.095191]\n",
            "Epoch 74 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=135152.295424]\n",
            "Epoch 75 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145176.724734]\n",
            "Epoch 75 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=134950.809766]\n",
            "Epoch 76 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144825.060450]\n",
            "Epoch 76 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=134837.442243]\n",
            "Epoch 77 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144512.958112]\n",
            "Epoch 77 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=134645.388783]\n",
            "Epoch 78 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144679.897274]\n",
            "Epoch 78 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=134459.808594]\n",
            "Epoch 79 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144668.010915]\n",
            "Epoch 79 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=134461.092522]\n",
            "Epoch 80 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144502.922207]\n",
            "Epoch 80 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=134298.800893]\n",
            "Epoch 81 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144085.887079]\n",
            "Epoch 81 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=134180.418638]\n",
            "Epoch 82 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143864.271277]\n",
            "Epoch 82 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=134070.374833]\n",
            "Epoch 83 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143760.265348]\n",
            "Epoch 83 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=134001.516295]\n",
            "Epoch 84 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143485.729832]\n",
            "Epoch 84 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=133996.508984]\n",
            "Epoch 85 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143538.612145]\n",
            "Epoch 85 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=133873.108594]\n",
            "Epoch 86 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143299.673426]\n",
            "Epoch 86 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=133734.129185]\n",
            "Epoch 87 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143476.478169]\n",
            "Epoch 87 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=133690.153181]\n",
            "Epoch 88 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143758.572196]\n",
            "Epoch 88 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=133647.088449]\n",
            "Epoch 89 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143201.973681]\n",
            "Epoch 89 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=133508.645982]\n",
            "Epoch 90 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=143464.763520]\n",
            "Epoch 90 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=133465.733259]\n",
            "Epoch 91 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143527.301751]\n",
            "Epoch 91 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=133366.458315]\n",
            "Epoch 92 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143168.935284]\n",
            "Epoch 92 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=133273.125837]\n",
            "Epoch 93 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142812.723183]\n",
            "Epoch 93 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=133221.520033]\n",
            "Epoch 94 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142931.463763]\n",
            "Epoch 94 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=133131.537500]\n",
            "Epoch 95 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142961.232214]\n",
            "Epoch 95 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=133075.621429]\n",
            "Epoch 96 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142397.392398]\n",
            "Epoch 96 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=133004.344364]\n",
            "Epoch 97 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142933.287511]\n",
            "Epoch 97 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=132968.903348]\n",
            "Epoch 98 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142284.445700]\n",
            "Epoch 98 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=132912.992076]\n",
            "Epoch 99 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142763.616633]\n",
            "Epoch 99 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=132816.672712]\n",
            "Epoch 100 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142223.024158]\n",
            "Epoch 100 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=132806.785435]\n",
            "Epoch 101 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142248.781472]\n",
            "Epoch 101 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132722.811440]\n",
            "Epoch 102 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142479.501164]\n",
            "Epoch 102 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=132616.981362]\n",
            "Epoch 103 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142479.062112]\n",
            "Epoch 103 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=132620.183761]\n",
            "Epoch 104 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142585.718750]\n",
            "Epoch 104 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132521.408650]\n",
            "Epoch 105 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142302.901374]\n",
            "Epoch 105 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132504.508929]\n",
            "Epoch 106 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142263.625055]\n",
            "Epoch 106 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132438.845815]\n",
            "Epoch 107 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141821.011746]\n",
            "Epoch 107 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132403.438170]\n",
            "Epoch 108 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142312.108599]\n",
            "Epoch 108 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132381.988504]\n",
            "Epoch 109 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142015.128989]\n",
            "Epoch 109 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=132351.536440]\n",
            "Epoch 110 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=142209.073194]\n",
            "Epoch 110 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=132320.154297]\n",
            "Epoch 111 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=142517.738531]\n",
            "Epoch 111 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=132277.601451]\n",
            "Epoch 112 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141909.971520]\n",
            "Epoch 112 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=132213.011886]\n",
            "Epoch 113 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141982.642509]\n",
            "Epoch 113 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=132126.219085]\n",
            "Epoch 114 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=142059.163952]\n",
            "Epoch 114 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=132107.557254]\n",
            "Epoch 115 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141723.475399]\n",
            "Epoch 115 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=132076.579241]\n",
            "Epoch 116 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141780.707502]\n",
            "Epoch 116 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=132072.581752]\n",
            "Epoch 117 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141648.080452]\n",
            "Epoch 117 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=132032.844252]\n",
            "Epoch 118 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141906.981882]\n",
            "Epoch 118 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=132019.458036]\n",
            "Epoch 119 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141432.585771]\n",
            "Epoch 119 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131977.608984]\n",
            "Epoch 120 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141468.991689]\n",
            "Epoch 120 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131967.382087]\n",
            "Epoch 121 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141939.089539]\n",
            "Epoch 121 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131962.183650]\n",
            "Epoch 122 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141553.028424]\n",
            "Epoch 122 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131922.921708]\n",
            "Epoch 123 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141513.152371]\n",
            "Epoch 123 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131912.885547]\n",
            "Epoch 124 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141553.348515]\n",
            "Epoch 124 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131869.483873]\n",
            "Epoch 125 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141543.694260]\n",
            "Epoch 125 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131843.107757]\n",
            "Epoch 126 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141436.350122]\n",
            "Epoch 126 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131842.037109]\n",
            "Epoch 127 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141577.042498]\n",
            "Epoch 127 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=131843.165681]\n",
            "Epoch 128 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141540.445645]\n",
            "Epoch 128 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131770.794531]\n",
            "Epoch 129 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141127.237256]\n",
            "Epoch 129 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=131761.791908]\n",
            "Epoch 130 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=140963.153369]\n",
            "Epoch 130 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131741.368471]\n",
            "Epoch 131 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141672.050698]\n",
            "Epoch 131 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131734.235770]\n",
            "Epoch 132 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=140991.552194]\n",
            "Epoch 132 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=131711.755859]\n",
            "Epoch 133 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141235.482934]\n",
            "Epoch 133 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=131702.117299]\n",
            "Epoch 134 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141474.190547]\n",
            "Epoch 134 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=131641.347433]\n",
            "Epoch 135 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141294.618351]\n",
            "Epoch 135 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=131634.808315]\n",
            "Epoch 136 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141252.840924]\n",
            "Epoch 136 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=131653.377846]\n",
            "Epoch 137 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141048.935561]\n",
            "Epoch 137 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131652.200000]\n",
            "Epoch 138 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141003.648715]\n",
            "Epoch 138 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131630.788170]\n",
            "Epoch 139 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141166.757369]\n",
            "Epoch 139 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131657.494085]\n",
            "Epoch 140 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141317.944204]\n",
            "Epoch 140 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=131571.611719]\n",
            "Epoch 141 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141201.133145]\n",
            "Epoch 141 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131593.203850]\n",
            "Epoch 142 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141433.041002]\n",
            "Epoch 142 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131566.167746]\n",
            "Epoch 143 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141414.488752]\n",
            "Epoch 143 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131561.337500]\n",
            "Epoch 144 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141314.877272]\n",
            "Epoch 144 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131579.757980]\n",
            "Epoch 145 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141159.408300]\n",
            "Epoch 145 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=131531.179576]\n",
            "Epoch 146 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141473.365470]\n",
            "Epoch 146 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131531.890179]\n",
            "Epoch 147 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141266.766678]\n",
            "Epoch 147 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=131542.416518]\n",
            "Epoch 148 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141021.988143]\n",
            "Epoch 148 val       : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=131506.814955]\n",
            "Epoch 149 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141087.481715]\n",
            "Epoch 149 val       : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=131505.800837]\n",
            "Epoch 150 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141064.415891]\n",
            "Epoch 150 val       : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=131496.249888]\n",
            "Epoch 151 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140975.217697]\n",
            "Epoch 151 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=131483.033538]\n",
            "Epoch 152 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141145.322196]\n",
            "Epoch 152 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131468.519643]\n",
            "Epoch 153 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141250.931793]\n",
            "Epoch 153 val       : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=131463.194699]\n",
            "Epoch 154 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141063.441822]\n",
            "Epoch 154 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131465.949386]\n",
            "Epoch 155 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141066.475510]\n",
            "Epoch 155 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131461.793638]\n",
            "Epoch 156 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141184.158743]\n",
            "Epoch 156 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131463.020815]\n",
            "Epoch 157 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140890.648881]\n",
            "Epoch 157 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131485.898438]\n",
            "Epoch 158 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141180.336658]\n",
            "Epoch 158 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131477.436161]\n",
            "Epoch 159 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140968.784796]\n",
            "Epoch 159 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131451.177790]\n",
            "Epoch 160 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140649.677748]\n",
            "Epoch 160 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131432.470815]\n",
            "Epoch 161 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140992.460993]\n",
            "Epoch 161 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131416.062667]\n",
            "Epoch 162 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141082.276651]\n",
            "Epoch 162 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131429.434598]\n",
            "Epoch 163 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141340.052471]\n",
            "Epoch 163 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131431.481138]\n",
            "Epoch 164 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141298.862145]\n",
            "Epoch 164 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131434.806138]\n",
            "Epoch 165 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141177.950521]\n",
            "Epoch 165 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131410.872600]\n",
            "Epoch 166 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140886.543828]\n",
            "Epoch 166 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131413.341406]\n",
            "Epoch 167 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141425.350953]\n",
            "Epoch 167 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131429.849944]\n",
            "Epoch 168 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141090.620789]\n",
            "Epoch 168 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131399.556417]\n",
            "Epoch 169 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140966.543384]\n",
            "Epoch 169 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131385.904911]\n",
            "Epoch 170 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141373.204344]\n",
            "Epoch 170 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131421.409933]\n",
            "Epoch 171 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141140.623227]\n",
            "Epoch 171 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131388.174219]\n",
            "Epoch 172 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140814.557458]\n",
            "Epoch 172 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131336.569252]\n",
            "Epoch 173 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141009.447307]\n",
            "Epoch 173 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131389.801618]\n",
            "Epoch 174 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141107.175643]\n",
            "Epoch 174 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131404.271708]\n",
            "Epoch 175 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140980.891401]\n",
            "Epoch 175 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131350.749554]\n",
            "Epoch 176 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141250.355330]\n",
            "Epoch 176 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131386.422991]\n",
            "Epoch 177 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141001.498338]\n",
            "Epoch 177 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131367.176674]\n",
            "Epoch 178 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140971.824690]\n",
            "Epoch 178 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131356.058036]\n",
            "Epoch 179 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140693.616800]\n",
            "Epoch 179 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131361.305525]\n",
            "Epoch 180 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140991.551640]\n",
            "Epoch 180 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131338.025781]\n",
            "Epoch 181 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140793.506261]\n",
            "Epoch 181 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131357.053516]\n",
            "Epoch 182 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141135.511248]\n",
            "Epoch 182 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131325.515737]\n",
            "Epoch 183 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141063.463930]\n",
            "Epoch 183 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131375.327455]\n",
            "Epoch 184 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140571.560173]\n",
            "Epoch 184 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131349.620871]\n",
            "Epoch 185 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140911.227781]\n",
            "Epoch 185 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131328.796373]\n",
            "Epoch 186 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140923.428801]\n",
            "Epoch 186 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131384.999554]\n",
            "Epoch 187 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140859.095135]\n",
            "Epoch 187 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131333.307645]\n",
            "Epoch 188 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141072.541777]\n",
            "Epoch 188 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131342.997489]\n",
            "Epoch 189 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141262.219914]\n",
            "Epoch 189 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131366.802958]\n",
            "Epoch 190 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141361.595024]\n",
            "Epoch 190 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131302.297210]\n",
            "Epoch 191 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140879.564993]\n",
            "Epoch 191 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131312.944364]\n",
            "Epoch 192 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140898.074801]\n",
            "Epoch 192 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131309.243862]\n",
            "Epoch 193 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141052.784353]\n",
            "Epoch 193 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131354.623605]\n",
            "Epoch 194 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141026.379820]\n",
            "Epoch 194 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131327.481306]\n",
            "Epoch 195 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141213.861148]\n",
            "Epoch 195 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131306.951004]\n",
            "Epoch 196 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140637.988808]\n",
            "Epoch 196 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131315.251395]\n",
            "Epoch 197 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140642.758311]\n",
            "Epoch 197 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131326.353069]\n",
            "Epoch 198 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141339.589151]\n",
            "Epoch 198 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131322.489676]\n",
            "Epoch 199 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140972.325632]\n",
            "Epoch 199 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131314.587946]\n",
            "Epoch 200 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140829.485151]\n",
            "Epoch 200 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131291.771261]\n",
            "Epoch 201 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140783.240248]\n",
            "Epoch 201 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131334.056083]\n",
            "Epoch 202 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141284.793994]\n",
            "Epoch 202 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131296.148270]\n",
            "Epoch 203 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141115.655530]\n",
            "Epoch 203 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131299.686440]\n",
            "Epoch 204 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141057.205452]\n",
            "Epoch 204 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131309.545536]\n",
            "Epoch 205 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140608.753989]\n",
            "Epoch 205 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131287.586272]\n",
            "Epoch 206 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141076.124889]\n",
            "Epoch 206 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131291.514118]\n",
            "Epoch 207 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=140863.474956]\n",
            "Epoch 207 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131293.569420]\n",
            "Epoch 208 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141199.194925]\n",
            "Epoch 208 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131279.696484]\n",
            "Epoch 209 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140993.221243]\n",
            "Epoch 209 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131314.268471]\n",
            "Epoch 210 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141033.433954]\n",
            "Epoch 210 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131278.018806]\n",
            "Epoch 211 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140778.328014]\n",
            "Epoch 211 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131304.847266]\n",
            "Epoch 212 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140664.584441]\n",
            "Epoch 212 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131293.953739]\n",
            "Epoch 213 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141100.190160]\n",
            "Epoch 213 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131283.973828]\n",
            "Epoch 214 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140755.337932]\n",
            "Epoch 214 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131301.112221]\n",
            "Epoch 215 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141130.248172]\n",
            "Epoch 215 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131291.593304]\n",
            "Epoch 216 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140669.182846]\n",
            "Epoch 216 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131252.437221]\n",
            "Epoch 217 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140841.761525]\n",
            "Epoch 217 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131289.055580]\n",
            "Epoch 218 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140874.138353]\n",
            "Epoch 218 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131291.637891]\n",
            "Epoch 219 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140850.357990]\n",
            "Epoch 219 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131306.998214]\n",
            "Epoch 220 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140709.574523]\n",
            "Epoch 220 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131297.377065]\n",
            "Epoch 221 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140669.771332]\n",
            "Epoch 221 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131286.402400]\n",
            "Epoch 222 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140832.501939]\n",
            "Epoch 222 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131307.301172]\n",
            "Epoch 223 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140829.465980]\n",
            "Epoch 223 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131243.231920]\n",
            "Epoch 224 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140915.172429]\n",
            "Epoch 224 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131248.193136]\n",
            "Epoch 225 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140993.408245]\n",
            "Epoch 225 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131263.469922]\n",
            "Epoch 226 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140771.055962]\n",
            "Epoch 226 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131291.336105]\n",
            "Epoch 227 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140930.413675]\n",
            "Epoch 227 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131265.654353]\n",
            "Epoch 228 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141035.260029]\n",
            "Epoch 228 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131251.528125]\n",
            "Epoch 229 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140809.457835]\n",
            "Epoch 229 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131277.843359]\n",
            "Epoch 230 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140906.109098]\n",
            "Epoch 230 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131264.785770]\n",
            "Epoch 231 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140725.604388]\n",
            "Epoch 231 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131260.434821]\n",
            "Epoch 232 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140748.347407]\n",
            "Epoch 232 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131273.386775]\n",
            "Epoch 233 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140981.288785]\n",
            "Epoch 233 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131280.783482]\n",
            "Epoch 234 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141087.549645]\n",
            "Epoch 234 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131294.815625]\n",
            "Epoch 235 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140706.743739]\n",
            "Epoch 235 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131275.218527]\n",
            "Epoch 236 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141009.157580]\n",
            "Epoch 236 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131287.248103]\n",
            "Epoch 237 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141059.162289]\n",
            "Epoch 237 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131292.880301]\n",
            "Epoch 238 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140540.806738]\n",
            "Epoch 238 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131263.494531]\n",
            "Epoch 239 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140986.904200]\n",
            "Epoch 239 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131247.997656]\n",
            "Epoch 240 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141101.074911]\n",
            "Epoch 240 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=131276.246205]\n",
            "Epoch 241 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141051.869625]\n",
            "Epoch 241 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131283.320257]\n",
            "Epoch 242 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140712.331062]\n",
            "Epoch 242 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=131253.793471]\n",
            "Epoch 243 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141126.068262]\n",
            "Epoch 243 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131311.289844]\n",
            "Epoch 244 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141124.319592]\n",
            "Epoch 244 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131288.345926]\n",
            "Epoch 245 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140967.019836]\n",
            "Epoch 245 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131256.890681]\n",
            "Epoch 246 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140811.333555]\n",
            "Epoch 246 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131306.272489]\n",
            "Epoch 247 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140692.527039]\n",
            "Epoch 247 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131276.779241]\n",
            "Epoch 248 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140924.386525]\n",
            "Epoch 248 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131250.284989]\n",
            "Epoch 249 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141026.202959]\n",
            "Epoch 249 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=131272.983092]\n",
            "Epoch 250 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140820.095191]\n",
            "Epoch 250 val       : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=131261.878906]\n",
            "Epoch 251 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140762.161348]\n",
            "Epoch 251 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131276.597210]\n",
            "Epoch 252 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140801.633256]\n",
            "Epoch 252 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131263.460268]\n",
            "Epoch 253 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140885.933677]\n",
            "Epoch 253 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131263.802846]\n",
            "Epoch 254 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140596.336159]\n",
            "Epoch 254 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131261.238672]\n",
            "Epoch 255 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140955.046598]\n",
            "Epoch 255 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131270.142187]\n",
            "Epoch 256 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140861.843473]\n",
            "Epoch 256 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131304.450000]\n",
            "Epoch 257 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140748.080341]\n",
            "Epoch 257 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131247.447154]\n",
            "Epoch 258 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140988.055075]\n",
            "Epoch 258 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131310.500446]\n",
            "Epoch 259 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140813.751828]\n",
            "Epoch 259 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=131254.941016]\n",
            "Epoch 260 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140517.406139]\n",
            "Epoch 260 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131283.163783]\n",
            "Epoch 261 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140839.784297]\n",
            "Epoch 261 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131247.923661]\n",
            "Epoch 262 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140647.253435]\n",
            "Epoch 262 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=131286.397154]\n",
            "Epoch 263 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140427.715758]\n",
            "Epoch 263 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131251.037612]\n",
            "Epoch 264 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140880.047097]\n",
            "Epoch 264 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131269.576786]\n",
            "Epoch 265 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141366.145058]\n",
            "Epoch 265 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131271.805246]\n",
            "Epoch 266 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140700.560727]\n",
            "Epoch 266 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131289.367801]\n",
            "Epoch 267 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140931.124778]\n",
            "Epoch 267 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131235.603683]\n",
            "Epoch 268 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141066.031638]\n",
            "Epoch 268 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131272.995982]\n",
            "Epoch 269 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140779.392841]\n",
            "Epoch 269 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131284.216629]\n",
            "Epoch 270 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140802.528978]\n",
            "Epoch 270 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131259.502679]\n",
            "Epoch 271 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140541.971520]\n",
            "Epoch 271 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131267.350558]\n",
            "Epoch 272 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140815.935782]\n",
            "Epoch 272 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131280.991518]\n",
            "Epoch 273 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141017.094637]\n",
            "Epoch 273 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131266.999163]\n",
            "Epoch 274 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141066.906527]\n",
            "Epoch 274 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131246.188672]\n",
            "Epoch 275 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140680.779034]\n",
            "Epoch 275 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131274.160882]\n",
            "Epoch 276 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140856.507757]\n",
            "Epoch 276 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131281.496652]\n",
            "Epoch 277 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141219.593307]\n",
            "Epoch 277 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131243.457143]\n",
            "Epoch 278 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140907.194149]\n",
            "Epoch 278 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131262.483594]\n",
            "Epoch 279 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140874.805131]\n",
            "Epoch 279 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131295.960379]\n",
            "Epoch 280 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141039.081727]\n",
            "Epoch 280 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131265.848047]\n",
            "Epoch 281 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140863.712877]\n",
            "Epoch 281 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131256.997042]\n",
            "Epoch 282 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141108.143783]\n",
            "Epoch 282 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131265.840513]\n",
            "Epoch 283 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140750.668606]\n",
            "Epoch 283 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131256.874219]\n",
            "Epoch 284 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141177.189218]\n",
            "Epoch 284 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=131249.915960]\n",
            "Epoch 285 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141008.330452]\n",
            "Epoch 285 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131271.803237]\n",
            "Epoch 286 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140692.310727]\n",
            "Epoch 286 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131271.564844]\n",
            "Epoch 287 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141023.152094]\n",
            "Epoch 287 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=131246.329576]\n",
            "Epoch 288 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141167.233378]\n",
            "Epoch 288 val       : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=131288.781194]\n",
            "Epoch 289 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140707.790392]\n",
            "Epoch 289 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131300.376786]\n",
            "Epoch 290 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140929.919770]\n",
            "Epoch 290 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131283.771317]\n",
            "Epoch 291 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140957.571476]\n",
            "Epoch 291 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131270.920871]\n",
            "Epoch 292 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=140984.467753]\n",
            "Epoch 292 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131272.248214]\n",
            "Epoch 293 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140930.918551]\n",
            "Epoch 293 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=131294.564397]\n",
            "Epoch 294 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140899.949246]\n",
            "Epoch 294 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131267.374107]\n",
            "Epoch 295 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141028.830895]\n",
            "Epoch 295 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131292.011161]\n",
            "Epoch 296 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141026.325078]\n",
            "Epoch 296 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131244.628683]\n",
            "Epoch 297 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140883.832779]\n",
            "Epoch 297 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131322.758371]\n",
            "Epoch 298 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140811.801252]\n",
            "Epoch 298 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131264.680692]\n",
            "Epoch 299 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140813.425477]\n",
            "Epoch 299 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131261.135826]\n",
            "Epoch 300 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=140751.979277]\n",
            "Epoch 300 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=131271.356306]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done! Time 2:44:01.702350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeCxA3hNIwLo",
        "outputId": "f001ed21-ad2d-4357-8428-e692a335b6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb1f2128198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbni37YoIwPC",
        "outputId": "1cca0016-c811-4463-bd2f-3c1da6ef74e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb1f202a470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZQzV4TqIwRm"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/together_0.1.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}