{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "together_dropout_0.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/together_dropout_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyAd8cKiv5Ej",
        "outputId": "c234a69e-319a-4d64-828c-488a722d59ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4wFefAvmwZdLWbJPKbdPRwJcV2eBAiNy1gJzRo613P_jXKSJS_D7yOA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB3d1IvI_Mpt"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/gdrive/My Drive/lungs/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80WH-bjfv5vK"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaWA_gNvxBJr"
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-bThzGw-vj"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8E20zma_baJ"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "\n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MepiJCewTlF"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self,TAB,A, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "        self.TAB=TAB\n",
        "        self.A=A\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image,\n",
        "            'tab':self.TAB[self.tensor_files[item].stem],\n",
        "            'slope':self.A[self.tensor_files[item].stem]\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPhMVZeywtjZ"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZEkwnKwyg9"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 10\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lVm3VSweak"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "# cum = 0\n",
        "# for i in range(len(train)):\n",
        "#     sample = train[i]['image']\n",
        "#     cum += torch.mean(sample).item()\n",
        "\n",
        "# assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTW4lnVLw2J1"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "\n",
        "        #tabular\n",
        "        self.tab1=nn.Linear(104,256)\n",
        "        self.tab2=nn.Linear(256,256)\n",
        "        self.tab3=nn.Linear(256,100)\n",
        "\n",
        "\n",
        "        #output\n",
        "        self.out1=nn.Linear(100,256)\n",
        "        self.out2=nn.Linear(256,256)\n",
        "        self.out3=nn.Linear(256,1)\n",
        "\n",
        "\n",
        "        #dropout\n",
        "        self.drop1=nn.Dropout(0.1)\n",
        "        self.drop2=nn.Dropout(0.1)\n",
        "        self.drop3=nn.Dropout(0.1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x,y, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        x = torch.cat((x,y),1)\n",
        "        x = self.act(self.tab1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.act(self.tab2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.act(self.tab3(x))\n",
        "\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4,x\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x,y):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4,out = self.encode(x,y)\n",
        "        \n",
        "        out1=self.act(self.out1(out))\n",
        "        out1=self.drop3(out1)\n",
        "        out2=self.act(self.out2(out1))\n",
        "        output=self.out3(out2)\n",
        "\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var,output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uggc0K_w4Z0"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNn3kzUZw4cU"
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(100):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            tabular=inputs['tab'].to(device, dtype=torch.float)\n",
        "            slopes_input=inputs['slope'].to(device, dtype=torch.float)\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var,slope_output = model(inputs,tabular)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                slope_loss = F.mse_loss(slope_output,slopes_input)\n",
        "                \n",
        "                loss =  reconst_loss + kl_div+slope_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkYBl9H52A9p"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OtoWy6n5L9i"
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaoFLisZsrAb"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/0.95_100.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1U87dFAtBlH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}