{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "together_dropout_linear_width_inc_l15.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/together_dropout_linear_width_inc_l15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "outputId": "9c230d04-eb59-4bad-b76d-0050250164f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/gdrive/My Drive/lungs/train.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "outputId": "a84de17f-c3d9-4c3d-dd83-3372e0d026c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "outputId": "0ff3f995-9bbd-46a0-d8d8-dccc0813f1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "\n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "176it [00:00, 1021.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self,TAB,A, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "        self.TAB=TAB\n",
        "        self.A=A\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image,\n",
        "            'tab':self.TAB[self.tensor_files[item].stem],\n",
        "            'slope':self.A[self.tensor_files[item].stem]\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGULd4YJatQz"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 15\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "# cum = 0\n",
        "# for i in range(len(train)):\n",
        "#     sample = train[i]['image']\n",
        "#     cum += torch.mean(sample).item()\n",
        "\n",
        "# assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "\n",
        "        #tabular\n",
        "        self.tab1=nn.Linear(104,512)\n",
        "        self.tab2=nn.Linear(512,512)\n",
        "        self.tab3=nn.Linear(512,100)\n",
        "\n",
        "\n",
        "        #output\n",
        "        self.out1=nn.Linear(100,512)\n",
        "        self.out2=nn.Linear(512,512)\n",
        "        self.out3=nn.Linear(512,1)\n",
        "\n",
        "\n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x,y, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        x = torch.cat((x,y),1)\n",
        "        x = self.act(self.tab1(x))\n",
        "        x = self.act(self.tab2(x))\n",
        "        x = self.tab3(x)\n",
        "\n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4,x\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x,y):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4,out = self.encode(x,y)\n",
        "        \n",
        "        out1=self.act(self.out1(out))\n",
        "        out2=self.act(self.out2(out1))\n",
        "        output=self.out3(out2)\n",
        "\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var,output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv",
        "outputId": "bb5f2314-d542-43a3-923e-b2349c09f335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(300):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            tabular=inputs['tab'].to(device, dtype=torch.float)\n",
        "            slopes_input=inputs['slope'].to(device, dtype=torch.float)\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var,slope_output = model(inputs,tabular)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                slope_loss = F.mse_loss(slope_output,slopes_input)\n",
        "                \n",
        "                loss =  reconst_loss + kl_div+slope_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 train       :   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       :  89%|████████▉ | 8/9 [00:31<00:03,  3.75s/it, loss=789853.781250]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       : 100%|██████████| 9/9 [00:34<00:00,  3.88s/it, loss=767180.689273]\n",
            "Epoch 1 val         :  67%|██████▋   | 2/3 [00:01<00:00,  1.06it/s, loss=660209.531250]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 val         : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=614254.068080]\n",
            "Epoch 2 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=605450.346188]\n",
            "Epoch 2 val         : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=553204.645312]\n",
            "Epoch 3 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=564188.216090]\n",
            "Epoch 3 val         : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=530686.163616]\n",
            "Epoch 4 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=537529.913785]\n",
            "Epoch 4 val         : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=494866.351786]\n",
            "Epoch 5 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=487139.538785]\n",
            "Epoch 5 val         : 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, loss=439007.842634]\n",
            "Epoch 6 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=428821.501330]\n",
            "Epoch 6 val         : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=369178.256250]\n",
            "Epoch 7 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=356874.067819]\n",
            "Epoch 7 val         : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=307887.242411]\n",
            "Epoch 8 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=307173.925864]\n",
            "Epoch 8 val         : 100%|██████████| 3/3 [00:01<00:00,  1.68it/s, loss=294097.227455]\n",
            "Epoch 9 train       : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=312823.134973]\n",
            "Epoch 9 val         : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=300417.434040]\n",
            "Epoch 10 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=305831.181738]\n",
            "Epoch 10 val        : 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, loss=286968.188504]\n",
            "Epoch 11 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=311966.467863]\n",
            "Epoch 11 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=301030.005357]\n",
            "Epoch 12 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=314419.323138]\n",
            "Epoch 12 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=300964.527790]\n",
            "Epoch 13 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=297283.603834]\n",
            "Epoch 13 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=271662.221317]\n",
            "Epoch 14 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=272185.599069]\n",
            "Epoch 14 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=247145.588504]\n",
            "Epoch 15 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=251013.272052]\n",
            "Epoch 15 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=228876.101228]\n",
            "Epoch 16 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=233420.606383]\n",
            "Epoch 16 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=213731.666071]\n",
            "Epoch 17 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=217856.721410]\n",
            "Epoch 17 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=201624.044420]\n",
            "Epoch 18 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=205253.017952]\n",
            "Epoch 18 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=188268.186161]\n",
            "Epoch 19 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=195843.122562]\n",
            "Epoch 19 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=188658.044420]\n",
            "Epoch 20 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=198159.170988]\n",
            "Epoch 20 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=188647.640290]\n",
            "Epoch 21 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=195232.641401]\n",
            "Epoch 21 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=183786.736161]\n",
            "Epoch 22 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=190041.827349]\n",
            "Epoch 22 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=179053.509598]\n",
            "Epoch 23 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=185901.374668]\n",
            "Epoch 23 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=175113.298772]\n",
            "Epoch 24 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=181302.835993]\n",
            "Epoch 24 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=170795.590402]\n",
            "Epoch 25 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=175653.849623]\n",
            "Epoch 25 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=166291.903013]\n",
            "Epoch 26 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=172584.606715]\n",
            "Epoch 26 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=163317.512277]\n",
            "Epoch 27 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=169666.560395]\n",
            "Epoch 27 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=160554.668192]\n",
            "Epoch 28 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=166495.977615]\n",
            "Epoch 28 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=158360.329353]\n",
            "Epoch 29 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=165275.675643]\n",
            "Epoch 29 val        : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=156485.681808]\n",
            "Epoch 30 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=162256.884364]\n",
            "Epoch 30 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=153059.384040]\n",
            "Epoch 31 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=157323.286126]\n",
            "Epoch 31 val        : 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, loss=147945.039621]\n",
            "Epoch 32 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=151442.068207]\n",
            "Epoch 32 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=143917.991071]\n",
            "Epoch 33 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=149778.485871]\n",
            "Epoch 33 val        : 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, loss=144597.154967]\n",
            "Epoch 34 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=151764.663010]\n",
            "Epoch 34 val        : 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, loss=146237.214397]\n",
            "Epoch 35 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=152590.544160]\n",
            "Epoch 35 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=145280.422545]\n",
            "Epoch 36 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=152130.152039]\n",
            "Epoch 36 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=146110.535100]\n",
            "Epoch 37 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=152265.211713]\n",
            "Epoch 37 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=144948.770480]\n",
            "Epoch 38 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=150316.636913]\n",
            "Epoch 38 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=143194.030915]\n",
            "Epoch 39 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=147962.909353]\n",
            "Epoch 39 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=140553.247042]\n",
            "Epoch 40 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145055.948748]\n",
            "Epoch 40 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=138581.227846]\n",
            "Epoch 41 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=145103.378602]\n",
            "Epoch 41 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=139495.103237]\n",
            "Epoch 42 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=145336.986148]\n",
            "Epoch 42 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=139110.545313]\n",
            "Epoch 43 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=144382.753491]\n",
            "Epoch 43 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=138033.160156]\n",
            "Epoch 44 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=143407.062943]\n",
            "Epoch 44 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=137031.691406]\n",
            "Epoch 45 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=142218.679798]\n",
            "Epoch 45 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=136875.282087]\n",
            "Epoch 46 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=143103.590204]\n",
            "Epoch 46 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=137173.627790]\n",
            "Epoch 47 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=142873.956006]\n",
            "Epoch 47 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=136204.886217]\n",
            "Epoch 48 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141600.957114]\n",
            "Epoch 48 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=135395.732813]\n",
            "Epoch 49 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141468.752272]\n",
            "Epoch 49 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=135447.187500]\n",
            "Epoch 50 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141414.894504]\n",
            "Epoch 50 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=135790.797935]\n",
            "Epoch 51 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=141252.401651]\n",
            "Epoch 51 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=135639.163728]\n",
            "Epoch 52 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=141175.197529]\n",
            "Epoch 52 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=135457.423549]\n",
            "Epoch 53 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=140796.774379]\n",
            "Epoch 53 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=134652.268080]\n",
            "Epoch 54 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=139805.877826]\n",
            "Epoch 54 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=133745.994531]\n",
            "Epoch 55 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=139213.870512]\n",
            "Epoch 55 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=133221.964453]\n",
            "Epoch 56 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=138321.818539]\n",
            "Epoch 56 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=132657.959375]\n",
            "Epoch 57 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=137587.293495]\n",
            "Epoch 57 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=132210.109877]\n",
            "Epoch 58 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=137415.504987]\n",
            "Epoch 58 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=131788.893304]\n",
            "Epoch 59 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=137109.779976]\n",
            "Epoch 59 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=131724.595089]\n",
            "Epoch 60 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=136597.419880]\n",
            "Epoch 60 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=131446.891685]\n",
            "Epoch 61 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=136583.468861]\n",
            "Epoch 61 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=130999.152121]\n",
            "Epoch 62 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=136535.968307]\n",
            "Epoch 62 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=130885.495647]\n",
            "Epoch 63 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=136269.580785]\n",
            "Epoch 63 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=130377.914955]\n",
            "Epoch 64 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=135671.066656]\n",
            "Epoch 64 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=130209.220536]\n",
            "Epoch 65 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=135748.145335]\n",
            "Epoch 65 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=130134.011830]\n",
            "Epoch 66 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=135182.634863]\n",
            "Epoch 66 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=130045.174275]\n",
            "Epoch 67 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=135207.684176]\n",
            "Epoch 67 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=129904.439732]\n",
            "Epoch 68 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=135179.752216]\n",
            "Epoch 68 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=129775.150893]\n",
            "Epoch 69 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=135142.985705]\n",
            "Epoch 69 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=129515.403348]\n",
            "Epoch 70 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=134300.904477]\n",
            "Epoch 70 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=129360.160770]\n",
            "Epoch 71 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=134412.545379]\n",
            "Epoch 71 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=129057.815569]\n",
            "Epoch 72 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=134350.474623]\n",
            "Epoch 72 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=128880.067634]\n",
            "Epoch 73 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=134575.489085]\n",
            "Epoch 73 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=129203.265458]\n",
            "Epoch 74 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=134513.753934]\n",
            "Epoch 74 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=128986.303404]\n",
            "Epoch 75 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=133996.383200]\n",
            "Epoch 75 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=128791.033036]\n",
            "Epoch 76 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=134401.468861]\n",
            "Epoch 76 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=128830.881027]\n",
            "Epoch 77 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=133433.001828]\n",
            "Epoch 77 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=128799.359766]\n",
            "Epoch 78 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=133896.599235]\n",
            "Epoch 78 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=128475.218583]\n",
            "Epoch 79 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=133566.378047]\n",
            "Epoch 79 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=128181.633259]\n",
            "Epoch 80 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132987.885084]\n",
            "Epoch 80 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=128127.289342]\n",
            "Epoch 81 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=133226.068816]\n",
            "Epoch 81 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=127831.263225]\n",
            "Epoch 82 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132940.948526]\n",
            "Epoch 82 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=127921.770145]\n",
            "Epoch 83 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=133123.271221]\n",
            "Epoch 83 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=127633.661551]\n",
            "Epoch 84 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132691.276817]\n",
            "Epoch 84 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=127439.836663]\n",
            "Epoch 85 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132726.638242]\n",
            "Epoch 85 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=127297.295201]\n",
            "Epoch 86 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132398.400488]\n",
            "Epoch 86 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=127355.045424]\n",
            "Epoch 87 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132676.433289]\n",
            "Epoch 87 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=127283.936719]\n",
            "Epoch 88 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132193.130375]\n",
            "Epoch 88 val        : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=127287.294587]\n",
            "Epoch 89 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=132332.265348]\n",
            "Epoch 89 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=127188.216685]\n",
            "Epoch 90 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=132228.010140]\n",
            "Epoch 90 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=127214.415458]\n",
            "Epoch 91 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132461.772274]\n",
            "Epoch 91 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=127309.474498]\n",
            "Epoch 92 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132286.330286]\n",
            "Epoch 92 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=127065.920926]\n",
            "Epoch 93 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=132093.671986]\n",
            "Epoch 93 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=127245.115067]\n",
            "Epoch 94 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131912.932957]\n",
            "Epoch 94 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=126878.774554]\n",
            "Epoch 95 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131916.834164]\n",
            "Epoch 95 val        : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126870.377902]\n",
            "Epoch 96 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131710.182513]\n",
            "Epoch 96 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=126701.572991]\n",
            "Epoch 97 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131475.973016]\n",
            "Epoch 97 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=126682.669754]\n",
            "Epoch 98 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131839.407192]\n",
            "Epoch 98 val        : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=126726.982924]\n",
            "Epoch 99 train      : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131780.933012]\n",
            "Epoch 99 val        : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=126588.366741]\n",
            "Epoch 100 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131530.608156]\n",
            "Epoch 100 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=126597.266239]\n",
            "Epoch 101 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131405.616966]\n",
            "Epoch 101 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126549.159598]\n",
            "Epoch 102 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131446.034353]\n",
            "Epoch 102 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=126428.674498]\n",
            "Epoch 103 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131623.952571]\n",
            "Epoch 103 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126356.974275]\n",
            "Epoch 104 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131204.070368]\n",
            "Epoch 104 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126236.961607]\n",
            "Epoch 105 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131414.093196]\n",
            "Epoch 105 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126252.070312]\n",
            "Epoch 106 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131574.339871]\n",
            "Epoch 106 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=126316.175502]\n",
            "Epoch 107 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=131300.090315]\n",
            "Epoch 107 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=126205.268750]\n",
            "Epoch 108 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131235.161458]\n",
            "Epoch 108 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126096.482310]\n",
            "Epoch 109 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131422.575244]\n",
            "Epoch 109 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125941.833147]\n",
            "Epoch 110 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130658.202959]\n",
            "Epoch 110 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=125810.202065]\n",
            "Epoch 111 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130731.407746]\n",
            "Epoch 111 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125890.002511]\n",
            "Epoch 112 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131019.866301]\n",
            "Epoch 112 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=126008.415123]\n",
            "Epoch 113 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130963.363752]\n",
            "Epoch 113 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125897.026060]\n",
            "Epoch 114 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131007.505264]\n",
            "Epoch 114 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125722.771596]\n",
            "Epoch 115 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130775.930796]\n",
            "Epoch 115 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125735.234040]\n",
            "Epoch 116 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130818.542110]\n",
            "Epoch 116 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125737.140123]\n",
            "Epoch 117 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130778.208887]\n",
            "Epoch 117 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125648.011272]\n",
            "Epoch 118 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130869.041168]\n",
            "Epoch 118 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125590.980357]\n",
            "Epoch 119 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130952.412788]\n",
            "Epoch 119 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125552.408594]\n",
            "Epoch 120 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130560.120180]\n",
            "Epoch 120 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125587.533371]\n",
            "Epoch 121 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130170.888298]\n",
            "Epoch 121 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125591.278795]\n",
            "Epoch 122 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130697.020833]\n",
            "Epoch 122 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125516.119475]\n",
            "Epoch 123 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130197.116301]\n",
            "Epoch 123 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125550.642634]\n",
            "Epoch 124 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130503.312001]\n",
            "Epoch 124 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125496.681696]\n",
            "Epoch 125 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130358.295878]\n",
            "Epoch 125 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125496.375837]\n",
            "Epoch 126 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130636.780585]\n",
            "Epoch 126 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125399.004743]\n",
            "Epoch 127 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130242.648992]\n",
            "Epoch 127 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=125424.561719]\n",
            "Epoch 128 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130305.797097]\n",
            "Epoch 128 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125379.980469]\n",
            "Epoch 129 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130159.074246]\n",
            "Epoch 129 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=125343.764174]\n",
            "Epoch 130 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130109.235982]\n",
            "Epoch 130 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125425.809040]\n",
            "Epoch 131 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130260.356937]\n",
            "Epoch 131 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125414.750446]\n",
            "Epoch 132 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130268.558843]\n",
            "Epoch 132 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125375.749554]\n",
            "Epoch 133 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130465.806184]\n",
            "Epoch 133 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=125314.218694]\n",
            "Epoch 134 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130112.333666]\n",
            "Epoch 134 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125226.705190]\n",
            "Epoch 135 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130215.855663]\n",
            "Epoch 135 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125187.015625]\n",
            "Epoch 136 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130230.939051]\n",
            "Epoch 136 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125275.749777]\n",
            "Epoch 137 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129967.804798]\n",
            "Epoch 137 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=125234.239955]\n",
            "Epoch 138 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130026.955397]\n",
            "Epoch 138 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125220.323605]\n",
            "Epoch 139 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130145.371177]\n",
            "Epoch 139 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125139.502455]\n",
            "Epoch 140 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130224.938608]\n",
            "Epoch 140 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125141.282924]\n",
            "Epoch 141 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130084.438885]\n",
            "Epoch 141 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125152.945536]\n",
            "Epoch 142 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130171.980552]\n",
            "Epoch 142 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125083.816518]\n",
            "Epoch 143 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130007.764129]\n",
            "Epoch 143 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125090.298270]\n",
            "Epoch 144 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130096.717642]\n",
            "Epoch 144 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125147.712054]\n",
            "Epoch 145 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130191.351950]\n",
            "Epoch 145 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=125082.874665]\n",
            "Epoch 146 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130189.595135]\n",
            "Epoch 146 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125096.044922]\n",
            "Epoch 147 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130529.715038]\n",
            "Epoch 147 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125107.028795]\n",
            "Epoch 148 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129774.810117]\n",
            "Epoch 148 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125099.745759]\n",
            "Epoch 149 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129809.899878]\n",
            "Epoch 149 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125125.387667]\n",
            "Epoch 150 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129869.245457]\n",
            "Epoch 150 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124998.497266]\n",
            "Epoch 151 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129849.370734]\n",
            "Epoch 151 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125046.555022]\n",
            "Epoch 152 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130226.473460]\n",
            "Epoch 152 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124988.168750]\n",
            "Epoch 153 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129838.429300]\n",
            "Epoch 153 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124978.086607]\n",
            "Epoch 154 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130382.667664]\n",
            "Epoch 154 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125031.929632]\n",
            "Epoch 155 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130134.849900]\n",
            "Epoch 155 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=125001.218694]\n",
            "Epoch 156 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129567.048426]\n",
            "Epoch 156 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125013.496429]\n",
            "Epoch 157 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129818.020889]\n",
            "Epoch 157 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125027.037221]\n",
            "Epoch 158 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130143.715980]\n",
            "Epoch 158 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=125001.855357]\n",
            "Epoch 159 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129739.837544]\n",
            "Epoch 159 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125023.548326]\n",
            "Epoch 160 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130065.325465]\n",
            "Epoch 160 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=125014.590904]\n",
            "Epoch 161 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129958.945700]\n",
            "Epoch 161 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124965.669587]\n",
            "Epoch 162 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130242.818816]\n",
            "Epoch 162 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=125006.855022]\n",
            "Epoch 163 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129714.473681]\n",
            "Epoch 163 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124982.484877]\n",
            "Epoch 164 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129906.757425]\n",
            "Epoch 164 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124975.851228]\n",
            "Epoch 165 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129797.435117]\n",
            "Epoch 165 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=125010.711719]\n",
            "Epoch 166 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129871.394171]\n",
            "Epoch 166 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124957.360993]\n",
            "Epoch 167 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129830.167442]\n",
            "Epoch 167 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124940.238728]\n",
            "Epoch 168 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129988.990027]\n",
            "Epoch 168 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124906.349888]\n",
            "Epoch 169 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130105.688387]\n",
            "Epoch 169 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124919.277511]\n",
            "Epoch 170 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130278.155862]\n",
            "Epoch 170 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124911.224609]\n",
            "Epoch 171 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129752.442431]\n",
            "Epoch 171 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=125024.013225]\n",
            "Epoch 172 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129731.557236]\n",
            "Epoch 172 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124990.819754]\n",
            "Epoch 173 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129699.605109]\n",
            "Epoch 173 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124947.626451]\n",
            "Epoch 174 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130030.420988]\n",
            "Epoch 174 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124918.100112]\n",
            "Epoch 175 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130066.354832]\n",
            "Epoch 175 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124925.619643]\n",
            "Epoch 176 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130146.031139]\n",
            "Epoch 176 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124923.392355]\n",
            "Epoch 177 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129900.231549]\n",
            "Epoch 177 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124913.285826]\n",
            "Epoch 178 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129956.314827]\n",
            "Epoch 178 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124946.500670]\n",
            "Epoch 179 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129509.661070]\n",
            "Epoch 179 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124946.800949]\n",
            "Epoch 180 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129823.091811]\n",
            "Epoch 180 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124910.423884]\n",
            "Epoch 181 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129961.074080]\n",
            "Epoch 181 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=124890.367411]\n",
            "Epoch 182 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129766.273992]\n",
            "Epoch 182 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124921.640123]\n",
            "Epoch 183 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129712.652981]\n",
            "Epoch 183 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124882.225725]\n",
            "Epoch 184 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129741.464650]\n",
            "Epoch 184 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=124922.300670]\n",
            "Epoch 185 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129788.059730]\n",
            "Epoch 185 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124904.076786]\n",
            "Epoch 186 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129754.144725]\n",
            "Epoch 186 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124940.403739]\n",
            "Epoch 187 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129702.697418]\n",
            "Epoch 187 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124882.029799]\n",
            "Epoch 188 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129642.696088]\n",
            "Epoch 188 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124898.319531]\n",
            "Epoch 189 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130001.646277]\n",
            "Epoch 189 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124893.883482]\n",
            "Epoch 190 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129692.598737]\n",
            "Epoch 190 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124880.843583]\n",
            "Epoch 191 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129860.018839]\n",
            "Epoch 191 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124934.066964]\n",
            "Epoch 192 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129899.796044]\n",
            "Epoch 192 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=124866.431529]\n",
            "Epoch 193 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129688.438442]\n",
            "Epoch 193 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124883.269420]\n",
            "Epoch 194 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130199.747839]\n",
            "Epoch 194 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124926.175614]\n",
            "Epoch 195 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130000.814772]\n",
            "Epoch 195 val       : 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, loss=124893.254743]\n",
            "Epoch 196 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129765.318152]\n",
            "Epoch 196 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124858.088449]\n",
            "Epoch 197 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130204.079676]\n",
            "Epoch 197 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124839.344754]\n",
            "Epoch 198 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129998.057735]\n",
            "Epoch 198 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124868.705748]\n",
            "Epoch 199 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130080.274601]\n",
            "Epoch 199 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124878.322656]\n",
            "Epoch 200 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129721.833112]\n",
            "Epoch 200 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124898.420815]\n",
            "Epoch 201 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129856.420213]\n",
            "Epoch 201 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124907.650614]\n",
            "Epoch 202 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129716.665503]\n",
            "Epoch 202 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124889.562723]\n",
            "Epoch 203 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129948.466977]\n",
            "Epoch 203 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124894.518973]\n",
            "Epoch 204 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129722.165836]\n",
            "Epoch 204 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124913.828460]\n",
            "Epoch 205 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129493.568816]\n",
            "Epoch 205 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124855.355413]\n",
            "Epoch 206 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129975.544991]\n",
            "Epoch 206 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124860.376395]\n",
            "Epoch 207 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129774.216035]\n",
            "Epoch 207 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124826.001953]\n",
            "Epoch 208 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129737.542609]\n",
            "Epoch 208 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124962.488393]\n",
            "Epoch 209 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129672.058067]\n",
            "Epoch 209 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124823.092857]\n",
            "Epoch 210 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130011.099291]\n",
            "Epoch 210 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124845.968248]\n",
            "Epoch 211 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129731.106937]\n",
            "Epoch 211 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=124876.733538]\n",
            "Epoch 212 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129794.853225]\n",
            "Epoch 212 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124866.140848]\n",
            "Epoch 213 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=130134.411292]\n",
            "Epoch 213 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124807.812835]\n",
            "Epoch 214 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129774.823969]\n",
            "Epoch 214 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124890.112723]\n",
            "Epoch 215 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129937.734209]\n",
            "Epoch 215 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124874.973270]\n",
            "Epoch 216 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129905.383921]\n",
            "Epoch 216 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124903.271150]\n",
            "Epoch 217 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129835.354333]\n",
            "Epoch 217 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124804.109989]\n",
            "Epoch 218 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129598.216201]\n",
            "Epoch 218 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124855.034375]\n",
            "Epoch 219 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129762.555962]\n",
            "Epoch 219 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124858.975391]\n",
            "Epoch 220 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129931.439107]\n",
            "Epoch 220 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124888.444643]\n",
            "Epoch 221 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129681.650654]\n",
            "Epoch 221 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124844.880190]\n",
            "Epoch 222 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129775.636469]\n",
            "Epoch 222 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=124865.710379]\n",
            "Epoch 223 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129986.348460]\n",
            "Epoch 223 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124839.076060]\n",
            "Epoch 224 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129761.101452]\n",
            "Epoch 224 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124892.130859]\n",
            "Epoch 225 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130144.246953]\n",
            "Epoch 225 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124891.462556]\n",
            "Epoch 226 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129785.155807]\n",
            "Epoch 226 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124813.177176]\n",
            "Epoch 227 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129601.560782]\n",
            "Epoch 227 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124868.854520]\n",
            "Epoch 228 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129957.034796]\n",
            "Epoch 228 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124893.312388]\n",
            "Epoch 229 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129647.206616]\n",
            "Epoch 229 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124831.334096]\n",
            "Epoch 230 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129897.316323]\n",
            "Epoch 230 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124828.516016]\n",
            "Epoch 231 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129842.297097]\n",
            "Epoch 231 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124844.137500]\n",
            "Epoch 232 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129962.282968]\n",
            "Epoch 232 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124813.771987]\n",
            "Epoch 233 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129249.840204]\n",
            "Epoch 233 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124827.285603]\n",
            "Epoch 234 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129941.459497]\n",
            "Epoch 234 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124897.116016]\n",
            "Epoch 235 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129842.056682]\n",
            "Epoch 235 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124880.749498]\n",
            "Epoch 236 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129915.996953]\n",
            "Epoch 236 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124880.157924]\n",
            "Epoch 237 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129440.744182]\n",
            "Epoch 237 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124855.798884]\n",
            "Epoch 238 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129933.042664]\n",
            "Epoch 238 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124846.858929]\n",
            "Epoch 239 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129654.879820]\n",
            "Epoch 239 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124844.350446]\n",
            "Epoch 240 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129897.715869]\n",
            "Epoch 240 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124856.309263]\n",
            "Epoch 241 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129682.671044]\n",
            "Epoch 241 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124847.478627]\n",
            "Epoch 242 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129771.301529]\n",
            "Epoch 242 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124854.422321]\n",
            "Epoch 243 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129881.214262]\n",
            "Epoch 243 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124854.490513]\n",
            "Epoch 244 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129990.009918]\n",
            "Epoch 244 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124814.515067]\n",
            "Epoch 245 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129454.624003]\n",
            "Epoch 245 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124905.624442]\n",
            "Epoch 246 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129633.949856]\n",
            "Epoch 246 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124928.284040]\n",
            "Epoch 247 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130036.507480]\n",
            "Epoch 247 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124852.061328]\n",
            "Epoch 248 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129826.399324]\n",
            "Epoch 248 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124808.518192]\n",
            "Epoch 249 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129761.578679]\n",
            "Epoch 249 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124823.876060]\n",
            "Epoch 250 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129858.638630]\n",
            "Epoch 250 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124830.216295]\n",
            "Epoch 251 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129853.884641]\n",
            "Epoch 251 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124788.666071]\n",
            "Epoch 252 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130044.999280]\n",
            "Epoch 252 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124791.195145]\n",
            "Epoch 253 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129705.350399]\n",
            "Epoch 253 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124897.587500]\n",
            "Epoch 254 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129678.889074]\n",
            "Epoch 254 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124868.047154]\n",
            "Epoch 255 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129701.100510]\n",
            "Epoch 255 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124897.897545]\n",
            "Epoch 256 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129738.482325]\n",
            "Epoch 256 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124826.326060]\n",
            "Epoch 257 train     : 100%|██████████| 9/9 [00:30<00:00,  3.42s/it, loss=129541.147773]\n",
            "Epoch 257 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124818.838449]\n",
            "Epoch 258 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130158.148770]\n",
            "Epoch 258 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124859.229241]\n",
            "Epoch 259 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129532.781527]\n",
            "Epoch 259 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124831.540234]\n",
            "Epoch 260 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129869.740304]\n",
            "Epoch 260 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124833.087556]\n",
            "Epoch 261 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129961.653923]\n",
            "Epoch 261 val       : 100%|██████████| 3/3 [00:01<00:00,  1.62it/s, loss=124782.508203]\n",
            "Epoch 262 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129883.233544]\n",
            "Epoch 262 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124864.982589]\n",
            "Epoch 263 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129886.095689]\n",
            "Epoch 263 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124870.826283]\n",
            "Epoch 264 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129943.571809]\n",
            "Epoch 264 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124852.051786]\n",
            "Epoch 265 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129614.721354]\n",
            "Epoch 265 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124844.088449]\n",
            "Epoch 266 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129723.827903]\n",
            "Epoch 266 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124826.525725]\n",
            "Epoch 267 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129958.264018]\n",
            "Epoch 267 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124835.370368]\n",
            "Epoch 268 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129773.637633]\n",
            "Epoch 268 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124881.206529]\n",
            "Epoch 269 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129814.883422]\n",
            "Epoch 269 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124820.251339]\n",
            "Epoch 270 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129807.517841]\n",
            "Epoch 270 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124823.888504]\n",
            "Epoch 271 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129803.614805]\n",
            "Epoch 271 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124826.195089]\n",
            "Epoch 272 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129781.976230]\n",
            "Epoch 272 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124801.075670]\n",
            "Epoch 273 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129889.837101]\n",
            "Epoch 273 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124815.282310]\n",
            "Epoch 274 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130001.780918]\n",
            "Epoch 274 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124813.020424]\n",
            "Epoch 275 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129946.663675]\n",
            "Epoch 275 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124828.064788]\n",
            "Epoch 276 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130025.149047]\n",
            "Epoch 276 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124817.106027]\n",
            "Epoch 277 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129725.993296]\n",
            "Epoch 277 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124861.193025]\n",
            "Epoch 278 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129529.272163]\n",
            "Epoch 278 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124906.635882]\n",
            "Epoch 279 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129354.326740]\n",
            "Epoch 279 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124867.157478]\n",
            "Epoch 280 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=130073.481161]\n",
            "Epoch 280 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124831.509654]\n",
            "Epoch 281 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129704.195700]\n",
            "Epoch 281 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124885.137221]\n",
            "Epoch 282 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130002.782746]\n",
            "Epoch 282 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124852.734487]\n",
            "Epoch 283 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129915.431904]\n",
            "Epoch 283 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124851.919643]\n",
            "Epoch 284 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130021.846576]\n",
            "Epoch 284 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=124921.635100]\n",
            "Epoch 285 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129980.262578]\n",
            "Epoch 285 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124816.979576]\n",
            "Epoch 286 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129573.867132]\n",
            "Epoch 286 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124853.604855]\n",
            "Epoch 287 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129635.894781]\n",
            "Epoch 287 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124875.399107]\n",
            "Epoch 288 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129923.089761]\n",
            "Epoch 288 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124816.547768]\n",
            "Epoch 289 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129779.128602]\n",
            "Epoch 289 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124851.261384]\n",
            "Epoch 290 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129738.740193]\n",
            "Epoch 290 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124802.935603]\n",
            "Epoch 291 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129769.959719]\n",
            "Epoch 291 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124806.407422]\n",
            "Epoch 292 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129809.259419]\n",
            "Epoch 292 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124791.666741]\n",
            "Epoch 293 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129942.487478]\n",
            "Epoch 293 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124799.867578]\n",
            "Epoch 294 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130062.683843]\n",
            "Epoch 294 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124917.097712]\n",
            "Epoch 295 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129649.011802]\n",
            "Epoch 295 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=124880.490904]\n",
            "Epoch 296 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129991.331837]\n",
            "Epoch 296 val       : 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=124839.778292]\n",
            "Epoch 297 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129750.630375]\n",
            "Epoch 297 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124806.105134]\n",
            "Epoch 298 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129910.318927]\n",
            "Epoch 298 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=124845.295313]\n",
            "Epoch 299 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129548.914229]\n",
            "Epoch 299 val       : 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, loss=124878.038114]\n",
            "Epoch 300 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129751.328125]\n",
            "Epoch 300 val       : 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, loss=124849.521819]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done! Time 2:44:00.139332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBMMJ3oMcLGH",
        "outputId": "049b05bb-aa40-47f1-ba7d-9b07ed767629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff704072b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcgaYQQ7cLDR",
        "outputId": "a7891c24-df5b-4ac6-d396-782e7f625aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff703f98710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfJElEQVR4nO3de5Bc5Z3e8e+vT1+nNZqLNBJCEkiAjA0EG1lhsdfxri0vCO9FJGEdnFRZu0tMNsaJ4y3HC3Fq8XrXSVjHxmbLxkUWYnARA1Z2g5JgYww4VCrFZTCYO2gsMJIQ0iBpdJlrX37547wzao3mzKDpGfXM9POpmprT73m7+z19Zvrp99Ld5u6IiIhMJNXoBoiIyNylkBARkUQKCRERSaSQEBGRRAoJERFJlG50A2ba0qVLfc2aNY1uhojIvPLUU0+97e5d48sXXEisWbOG7u7uRjdDRGReMbNfTVSu4SYREUmkkBARkUQKCRERSaSQEBGRRAoJERFJpJAQEZFECgkREUmkkAgeemkvt/zsl41uhojInKKQCB59tZfv/h+FhIhILYVEkM9GDI5UGt0MEZE5RSERtGTSjFSqlCvVRjdFRGTOUEgELdkIgMGSehMiIqMUEkF+NCQ05CQiMmbKkDCz281sn5k9X1P2NTN72cyeNbO/M7P2mn3Xm1mPmb1iZpfVlG8KZT1mdl1N+VozezyU32Nm2VCeC5d7wv41M3XQE2nJqCchIjLeO+lJfA/YNK7sQeACd78QeBW4HsDMzgOuAs4P1/mOmUVmFgHfBi4HzgM+GeoC3Ajc5O7nAAeBq0P51cDBUH5TqDdrRoebBtSTEBEZM2VIuPujwIFxZT9x93K4+BiwKmxvBu5292F3fw3oAS4OPz3uvsPdR4C7gc1mZsBHga3h+ncAV9Tc1h1heyuwMdSfFQWFhIjICWZiTuKPgB+F7ZXAzpp9u0JZUvkSoK8mcEbLj7utsP9QqH8CM7vGzLrNrLu3t3daB1HIaE5CRGS8ukLCzL4ElIG7ZqY50+Put7r7Bnff0NV1wrfvvSMt2fhL+jQnISJyzLS/vtTM/gD4HWCju3so3g2srqm2KpSRUL4faDezdOgt1NYfva1dZpYG2kL9WXFsuKk8RU0RkeYxrZ6EmW0Cvgj8nrsP1OzaBlwVViatBdYBTwBPAuvCSqYs8eT2thAujwBXhutvAe6rua0tYftK4OGaMJpxBS2BFRE5wZQ9CTP7AfCbwFIz2wXcQLyaKQc8GOaSH3P3P3b3F8zsXuBF4mGoa929Em7ns8ADQATc7u4vhLv4U+BuM/tL4GngtlB+G/B9M+shnji/agaON5GWwIqInGjKkHD3T05QfNsEZaP1vwp8dYLy+4H7JyjfQbz6aXz5EPD7U7Vvpmh1k4jIifSO6yCXTpEyDTeJiNRSSARmRiETabhJRKSGQqJGIZvWcJOISA2FRI2WbMSglsCKiIxRSNRoyUbqSYiI1FBI1MhrTkJE5DgKiRot+gpTEZHjKCRqaLhJROR4Coka+UzEkIabRETGKCRqqCchInI8hUSNlmya/mEtgRURGaWQqLF8cZ4jw2UFhYhIoJCocXp7HoA9hwYb3BIRkblBIVHj9PYCALv7hhrcEhGRuUEhUWNFW+hJ9KknISICConjLF+cxwzePKSehIgIKCSOk4lSLG/N86Z6EiIigELiBCva85q4FhEJFBLjnN5WYI8mrkVEAIXECVa05dmjOQkREUAhcYKOYpbBUkWf4SQigkLiBB0tWQD6BkoNbomISOMpJMbpaMkAcKB/pMEtERFpPIXEOO1jPQmFhIiIQmKcjmLckzio4SYREYXEeKNzEgfVkxARUUiM1x7mJDTcJCKikDhBLh3Rko003CQigkJiQh0tWQ03iYigkJhQe0tG75MQEUEhMSH1JEREYlOGhJndbmb7zOz5mrJOM3vQzLaH3x2h3MzsZjPrMbNnzWx9zXW2hPrbzWxLTfn7zey5cJ2bzcwmu49TQT0JEZHYO+lJfA/YNK7sOuAhd18HPBQuA1wOrAs/1wC3QPyED9wA/BpwMXBDzZP+LcCna663aYr7mHXqSYiIxKYMCXd/FDgwrngzcEfYvgO4oqb8To89BrSb2QrgMuBBdz/g7geBB4FNYd9id3/M3R24c9xtTXQfs66tkOHwYIm4SSIizWu6cxLL3X1P2H4LWB62VwI7a+rtCmWTle+aoHyy+ziBmV1jZt1m1t3b2zuNwzleMZem6jBcrtZ9WyIi81ndE9ehBzCrL7mnug93v9XdN7j7hq6urrrvr5iLADg6XK77tkRE5rPphsTeMFRE+L0vlO8GVtfUWxXKJitfNUH5ZPcx64rZNAADw/pOCRFpbtMNiW3A6AqlLcB9NeWfCqucLgEOhSGjB4BLzawjTFhfCjwQ9h02s0vCqqZPjbutie5j1qknISISS09Vwcx+APwmsNTMdhGvUvpPwL1mdjXwK+ATofr9wMeBHmAA+EMAdz9gZn8BPBnqfcXdRyfDP0O8gqoA/Cj8MMl9zLpiLn5Y+kcUEiLS3KYMCXf/ZMKujRPUdeDahNu5Hbh9gvJu4IIJyvdPdB+nQksYbupXT0JEmpzecT2BRaM9Cc1JiEiTU0hMYHROQsNNItLsFBITKGq4SUQEUEhMaHTiemBEw00i0twUEhPIplNkItMSWBFpegqJBMVcmgGFhIg0OYVEgmI2zVGtbhKRJqeQSFDMRQxodZOINDmFRIKWbFpzEiLS9BQSCRbl0lrdJCJNTyGRoCUb6X0SItL0FBIJFuXSese1iDQ9hUSCYi6tz24SkaankEjQkos0cS0iTU8hkaAlk2akXKVSndVvZhURmdMUEgnymfihGS5ryElEmpdCIkE+E39c+KCWwYpIE1NIJBjtSQyVqw1uiYhI4ygkEoz2JIZK6kmISPNSSCTIpRUSIiIKiQRjw00lDTeJSPNSSCQohOGmYfUkRKSJKSQSjM1JaAmsiDQxhUSCY0tgNdwkIs1LIZHg2JyEehIi0rwUEgk03CQiopBIlB9bAqvhJhFpXgqJBDkNN4mIKCSS5NIpzLQEVkSam0IigZmRT0cMKiREpIkpJCaRz6Q0JyEiTa2ukDCzz5vZC2b2vJn9wMzyZrbWzB43sx4zu8fMsqFuLlzuCfvX1NzO9aH8FTO7rKZ8UyjrMbPr6mnrdOQzkeYkRKSpTTskzGwl8K+BDe5+ARABVwE3Aje5+znAQeDqcJWrgYOh/KZQDzM7L1zvfGAT8B0zi8wsAr4NXA6cB3wy1D1l8plIHxUuIk2t3uGmNFAwszTQAuwBPgpsDfvvAK4I25vDZcL+jWZmofxudx9299eAHuDi8NPj7jvcfQS4O9Q9ZXLplHoSItLUph0S7r4b+M/AG8ThcAh4Cuhz93KotgtYGbZXAjvDdcuh/pLa8nHXSSo/gZldY2bdZtbd29s73UM6gYabRKTZ1TPc1EH8yn4tcDpQJB4uOuXc/VZ33+DuG7q6umbsduOJa4WEiDSveoabPga85u697l4C/hb4daA9DD8BrAJ2h+3dwGqAsL8N2F9bPu46SeWnTCETaXWTiDS1ekLiDeASM2sJcwsbgReBR4ArQ50twH1he1u4TNj/sLt7KL8qrH5aC6wDngCeBNaF1VJZ4sntbXW096RpuElEml166ioTc/fHzWwr8HOgDDwN3Ar8b+BuM/vLUHZbuMptwPfNrAc4QPykj7u/YGb3EgdMGbjW3SsAZvZZ4AHilVO3u/sL023vdMSrmxQSItK8ph0SAO5+A3DDuOIdxCuTxtcdAn4/4Xa+Cnx1gvL7gfvraWM99GY6EWl2esf1JHJpDTeJSHNTSExCcxIi0uwUEpMoZCJKFadc0ZCTiDQnhcQkCtnwnRL6aA4RaVIKiUkUwleYDo5oyElEmpNCYhKFbLz4SyEhIs1KITGJsZ6EJq9FpEkpJCbRko1DYmCkPEVNEZGFSSExibx6EiLS5BQSkyiEnoTeKyEizUohMYljw00KCRFpTgqJSWgJrIg0O4XEJDTcJCLNTiExidGehIabRKRZKSQmodVNItLsFBKTiFJGLp3SnISINC2FxBQK2Ug9CRFpWgqJKbRkIvUkRKRpKSSmkM9GDKgnISJNSiExhUImYkg9CRFpUgqJKbRkIy2BFZGmpZCYQj6jiWsRaV4KiSm0ZCO941pEmpZCYgqFjIabRKR5KSSmoPdJiEgzU0hMoZBJa3WTiDQthcQUCtkUA6UK7t7opoiInHIKiSkUc2kqVac/9CaGShX+w/0v8c/veFIT2iKy4CkkprBuWSsAr7x1GIB7ntzJrY/u4Kcv7eONAwONbJqIyKxLN7oBc90FKxcD8D+efpMHXthL/3B5bN/+oyOwvFEtExGZfQqJKZy2OE9nMcv3H/sVACvbC2TTKUbKVQ70jzS4dSIis6uu4SYzazezrWb2spm9ZGYfMLNOM3vQzLaH3x2hrpnZzWbWY2bPmtn6mtvZEupvN7MtNeXvN7PnwnVuNjOrp73TPEbOP33x2OXdfYNsOLMDgAP9w6e6OSIip1S9cxLfAn7s7u8G3gu8BFwHPOTu64CHwmWAy4F14eca4BYAM+sEbgB+DbgYuGE0WEKdT9dcb1Od7Z2WC1a2ATAaURed0Q7AfvUkRGSBm3ZImFkb8GHgNgB3H3H3PmAzcEeodgdwRdjeDNzpsceAdjNbAVwGPOjuB9z9IPAgsCnsW+zuj3m8/vTOmts6pbZ8YA1/9Y8vHOtRnLNsEW2FTDwnISKygNXTk1gL9AL/1cyeNrO/MbMisNzd94Q6b3FsanclsLPm+rtC2WTluyYoP4GZXWNm3WbW3dvbW8chTey0tjyf+PurOX9F3KNYs6TIkkVZzUmIyIJXT0ikgfXALe5+EdDPsaElAEIPYNbfhebut7r7Bnff0NXVNWv384Gzl1DMRpy9bBFLiln2a05CRBa4ekJiF7DL3R8Pl7cSh8beMFRE+L0v7N8NrK65/qpQNln5qgnKG2bz+07n8S99jMX5DJ1F9SREZOGbdki4+1vATjM7NxRtBF4EtgGjK5S2APeF7W3Ap8Iqp0uAQ2FY6gHgUjPrCBPWlwIPhH2HzeySsKrpUzW31RBmxqJcvGp4yaKcQkJEFrx63yfxr4C7zCwL7AD+kDh47jWzq4FfAZ8Ide8HPg70AAOhLu5+wMz+Angy1PuKux8I258BvgcUgB+FnzlhSehJVKtOKnXKV+aKiJwSdYWEuz8DbJhg18YJ6jpwbcLt3A7cPkF5N3BBPW2cLZ3FLFWHvsESncVso5sjIjIr9NlN07RkUQ6At49q8lpEFi6FxDQta41DYt9hhYSILFwKiWlavjgPwL4jQw1uiYjI7FFITNNYT+KIehIisnApJKapmEtTzEbsPayehIgsXAqJOixbnFdPQkQWNIVEHZa15ujVxLWILGAKiTosW5xnryauRWQBU0jUYVlrjn2Hh4nfJygisvAoJOqwrDXHYKnC0ZrvvRYRWUgUEnUYfa+EVjiJyEKlkKjDqo4CADsPDja4JSIis0MhUYczOlsA2HlgoMEtERGZHQqJOnS15silUwoJEVmwFBJ1MDNWdRTYeUDDTSKyMCkk6nRGZwtvqCchIguUQqJOqztb2HlQISEiC5NCok6rO1o4MlTm0ECp0U0REZlxCok6nbEkXuH0+v7+BrdERGTmKSTqdHZXEYAdbx9tcEtERGaeQqJOZ3QWiVLGL/epJyEiC49Cok7ZdIozOlvUkxCRBUkhMQPO7iqqJyEiC5JCYgac3bWI1/b3U6nqI8NFZGFRSMyAs7qKjJSr7NL7JURkgVFIzIBzlrUCsH2v5iVEZGFRSMyAdy1fBMAre480uCUiIjNLITEDWvMZVrYXeOUthYSILCwKiRny7tNaFRIisuAoJGbIu05r5Ze9RxkpVxvdFBGRGaOQmCHnLm+lXHV9hpOILCh1h4SZRWb2tJn9r3B5rZk9bmY9ZnaPmWVDeS5c7gn719TcxvWh/BUzu6ymfFMo6zGz6+pt62w697R4hdPLGnISkQVkJnoSnwNeqrl8I3CTu58DHASuDuVXAwdD+U2hHmZ2HnAVcD6wCfhOCJ4I+DZwOXAe8MlQd046qyv+DKdXFRIisoDUFRJmtgr4beBvwmUDPgpsDVXuAK4I25vDZcL+jaH+ZuBudx9299eAHuDi8NPj7jvcfQS4O9Sdk3LpiLVLi1oGKyILSr09iW8CXwRGZ2uXAH3uXg6XdwErw/ZKYCdA2H8o1B8rH3edpPI569yaFU5HhvQlRCIy/007JMzsd4B97v7UDLZnum25xsy6zay7t7e3Ye04d3krbxwYYOPXf8aFf/4THn21cW0REZkJ9fQkfh34PTN7nXgo6KPAt4B2M0uHOquA3WF7N7AaIOxvA/bXlo+7TlL5Cdz9Vnff4O4burq66jik+px/+mIA3j46wvLWPDf++GWq+tA/EZnHph0S7n69u69y9zXEE88Pu/s/Ax4BrgzVtgD3he1t4TJh/8Pu7qH8qrD6aS2wDngCeBJYF1ZLZcN9bJtue0+Fj5y7jDv/6GIe/3cb+beXncsLbx7mgRfeanSzRESmbTbeJ/GnwJ+YWQ/xnMNtofw2YEko/xPgOgB3fwG4F3gR+DFwrbtXwrzFZ4EHiFdP3RvqzlmplPHhd3WRz0RccdFKzlm2iK8/+Ko+QlxE5i2LX8wvHBs2bPDu7u5GNwOA+5/bw2fu+jnf+MR7+UfrVzW6OSIiiczsKXffML5c77ieRZvOP43zVizmmz/dTqmij+sQkflHITGLUinjC5e9izcODPDD7l2Nbo6IyElTSMyyj5y7jPVntPPXD29nqFRpdHNERE6KQmKWmRlfuPRc9hwa4q7H32h0c0RETopC4hT44DlL+eDZS7jlZz30D5envoKIyByhkDhFvnDZubx9dISv/M8XWWgrykRk4VJInCLrz+jgsx85h3u6d/Kdn/2y0c0REXlH0lNXkZny+d+KVzp97YFXSKeMf/EbZze6SSIik1JInEJRyvjGJ95L1Z3/+KOX6R+p8PmPrSP+xHQRkblHIXGKpaMU3/wn7yOfibj5oe0Us5F6FCIyZ2lOogHSUYqvXXkhH3vPMm5+aDt7Dw81ukkiIhNSSDSImfHvf/s8hstVbn10R6ObIyIyIYVEA61ZWmTTBafxw+6dDIzo/RMiMvcoJBpsywfXcHiozNan9NlOIjL3aOK6wTac2cHFazr51k+301bI8NcP91DMpdnRe5Rzli3iy797Pu9d3d7oZopIk1JPosHMjD/73fM4MDDC5+5+hqo7uPPhdV3s6Rviyu/+P/UyRKRh1JOYAy5Y2cbWP/4gw6UK68/sIJ+JAOgbGOEzd/2cL279BZnIeO+qdvoGS5y7vJVCNmpwq0WkGeib6ea4gZEyf3D7kzzx+oGxss5ilk//g7P42HuWYQantRVYlFPei8j0JX0znUJiHhgqVbjxxy+zOJ/hPSta+W9P7OTRV3vH9ufSKS47/zQuOqOdHb399I+UWbeslXXLFnHh6jaWteYb2HoRmQ8UEgvM87sP8drb/VTd6X79INt+8SaHBksUsxHFXJp9R4YBSBlcdEYHf29lGx86ZylnL1vEqo4CmUjTUSJyjEJigRspVzk6XGZxPk06SnFosETPviM88nIvj+3Yz3O7DzFcjr9nO50yVnUUOL392M+q9gLLFudozWco5iJaMmmKuYjOYlafLSXSBJJCQgPZC0Q2naIznR273FbI8P4zO3n/mZ1APLfx4puHee3tfl7f38/r+wfY0zfI/93+NnuPDJH0WqGQiVjRniedOhYUKTNOa8uzPAxjLW3NsqKtQGcxS3tLhvZC+N2SoZCJFDIi85hCokm0ZNNsWNPJhjWdJ+wbKVfZe3iIfUeGODJUpn+4wlCpwqHBErsODrL38FC8NDcoVZy9h4d48c3DAOzvH6FSTe6RZtMpcukUpUqVyIx8JqLqTqniuDvL2/IsXZQjn4nIhbrHtiNymbgsm06RThlVj4fR0qkUmXSKbGQTbmciIxOlxobWKlVnUS5N38AIncU4UB2oulOtMnaMmShFOjLSKSNKxbcX/zZSo2Hp4Dju8W24O6MPQVshA0D/cJn+kTKdxSwt2TTVqlOuOpWqU6pWyUapsZVs7s5wuYo7pFLxsaWMKQO2UnWGyxVasif+K7s7AyMVCpnoWLtrrheljGrVT9g3m0ZHLvTCoT7ufsoeQ4WEkE2nWN3ZwurOlmldv1J1eo8Mc3BghL6BEn0DI/QNlugbKDFYqjBcrjBcqpJNp6hUnaFSZezJF2B33wB9AyUOD5YYLlcZLlUYLlcZKsVhNVKpUqrM72HRlMFEOZqNUqRSjAXEeFHKiCwOqyhljD0vhLqDpQrlqpOJ7LgnDSM+L+WqYxb3CI34yTllcHiozKJcmqPDZdIpo5CNyGciBkcqGBBFhjtUq07FfSxIR7ezUYqWbETKbCxo3Y/9dg/Xq8ZhCuAO5RBOi/NpsukU5XBec+kUUWRjYV0NoVutHtvORMbifOZY0Faq8fEBxXAsR4fL8QuKKMWR4TKFTDS28q8SbgsYexwNo+I+FvLuTsriFwOjj3ut0ZDzscujp8PHXT5+P+P2E85FlIpfpA2VquQzKY4Ol4lS8YubbJTCDAZG4v+HbHjxMlyqMlKp0laIe+ujj8VIucq3/+l6PnjO0hP/kOqgkJC6Ral4+Om0ttlbRVWtOiPhSWH0Cbdcif9ZypX4n6Q09vv47XLFcRzDODJcpqMlw8GBEkY8dDb6in30+aA8+gRUiZ+MKh56AOH+Ia4fP+nGt2EWPzE70DdQImXxE1chE/H20WGGStWa3kncIxqpVDk8VAKPnyRzmfhJt+o+9gQ/2vuoulOuxL9rn+BymRSt+TSHB4999pfH3RyilNGazzBYqjAQvlu9Em6no5jl8GCJxYUMpUqVwZE4kEfff1MJT76jT5aplI09VimL2z44UqEanlSPfxzi31HNdUaloxTVqnNosMRIOX5MIH6ijM9t/MQZ31Z83dHbGS7Hj1c6vMBIp4x0FIdU/3CZ1nyaYjbNcDn+u2jNpRkqVTg6XAnti2937HEKvcE4fMPfAYYTznvVqVSPtf3Y4z7+sh1/eewuLPF6caDGoZOOjHw6YqhcoTV/7Em/VKlSqUIxG/emSxVnpFylkI1Ip4y+gRKHBkukIyMbesxLW3Pv5N/ppCgkZF5IpYx8Sm8gFDnVtA5SREQSKSRERCSRQkJERBIpJEREJJFCQkREEikkREQkkUJCREQSKSRERCTRgvsUWDPrBX41zasvBd6eweY0ko5lbtKxzE06FjjT3bvGFy64kKiHmXVP9FG585GOZW7SscxNOpZkGm4SEZFECgkREUmkkDjerY1uwAzSscxNOpa5SceSQHMSIiKSSD0JERFJpJAQEZFEConAzDaZ2Stm1mNm1zW6PSfLzF43s+fM7Bkz6w5lnWb2oJltD787Gt3OiZjZ7Wa2z8yerymbsO0Wuzmcp2fNbH3jWn68hOP4spntDuflGTP7eM2+68NxvGJmlzWm1RMzs9Vm9oiZvWhmL5jZ50L5fDwvSccy786NmeXN7Akz+0U4lj8P5WvN7PHQ5nvMLBvKc+FyT9i/5qTv1MP3uzbzDxABvwTOArLAL4DzGt2ukzyG14Gl48r+CrgubF8H3Njodia0/cPAeuD5qdoOfBz4EfG3QV4CPN7o9k9xHF8GvjBB3fPC31kOWBv+/qJGH0NN+1YA68N2K/BqaPN8PC9JxzLvzk14fBeF7QzweHi87wWuCuXfBf5l2P4M8N2wfRVwz8nep3oSsYuBHnff4e4jwN3A5ga3aSZsBu4I23cAVzSwLYnc/VHgwLjipLZvBu702GNAu5mtODUtnVzCcSTZDNzt7sPu/hrQQ/x3OCe4+x53/3nYPgK8BKxkfp6XpGNJMmfPTXh8j4aLmfDjwEeBraF8/HkZPV9bgY1mNV/0/Q4oJGIrgZ01l3cx+R/RXOTAT8zsKTO7JpQtd/c9YfstYHljmjYtSW2fj+fqs2EI5vaaIb95cxxhiOIi4let8/q8jDsWmIfnxswiM3sG2Ac8SNzT6XP3cqhS296xYwn7DwFLTub+FBILx4fcfT1wOXCtmX24dqfH/c15ud55PrcduAU4G3gfsAf4emObc3LMbBHw34F/4+6Ha/fNt/MywbHMy3Pj7hV3fx+wiriH8+7ZvD+FRGw3sLrm8qpQNm+4++7wex/wd8R/PHtHu/zh977GtfCkJbV9Xp0rd98b/qmrwH/h2LDFnD8OM8sQP6ne5e5/G4rn5XmZ6Fjm87kBcPc+4BHgA8TDe+mwq7a9Y8cS9rcB+0/mfhQSsSeBdWGFQJZ4gmdbg9v0jplZ0cxaR7eBS4HniY9hS6i2BbivMS2clqS2bwM+FVbTXAIcqhn+mHPGjcv/Q+LzAvFxXBVWn6wF1gFPnOr2JQnj1rcBL7n7N2p2zbvzknQs8/HcmFmXmbWH7QLwW8RzLI8AV4Zq48/L6Pm6Eng49ADfuUbP1s+VH+LVGa8Sj+99qdHtOcm2n0W8GuMXwAuj7Scee3wI2A78FOhsdFsT2v8D4u5+iXg89eqkthOv7vh2OE/PARsa3f4pjuP7oZ3Phn/YFTX1vxSO4xXg8ka3f9yxfIh4KOlZ4Jnw8/F5el6SjmXenRvgQuDp0ObngT8L5WcRB1kP8EMgF8rz4XJP2H/Wyd6nPpZDREQSabhJREQSKSRERCSRQkJERBIpJEREJJFCQkREEikkREQkkUJCREQS/X8KgNOnf7eBvQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvZEWNQUcLAl"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/l15.pt')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UefI1PrrcK9d"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdMD3yGcK6g"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}