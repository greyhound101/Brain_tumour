{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lr_tune_3e-3_0.96.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/lr_tune_3e_3_0_96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyAd8cKiv5Ej",
        "outputId": "6429d80e-32ed-4f68-8691-2820f76228eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80WH-bjfv5vK"
      },
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaWA_gNvxBJr",
        "outputId": "44c5eafa-8d41-4abe-b95f-8d7006a1f730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-bThzGw-vj"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MepiJCewTlF"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPhMVZeywtjZ"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZEkwnKwyg9"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 10\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lVm3VSweak"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "cum = 0\n",
        "for i in range(len(train)):\n",
        "    sample = train[i]['image']\n",
        "    cum += torch.mean(sample).item()\n",
        "\n",
        "assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTW4lnVLw2J1"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        \n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4 = self.encode(x)\n",
        "        \n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uggc0K_w4Z0"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNn3kzUZw4cU",
        "outputId": "775b5429-3258-4c37-fd1f-7587e1c1a0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(100):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var = model(inputs)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                \n",
        "                loss =  reconst_loss + kl_div\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 train       :   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Epoch 1 train       : 100%|██████████| 9/9 [00:28<00:00,  3.14s/it, loss=1370690.864362]\n",
            "Epoch 1 val         : 100%|██████████| 3/3 [00:03<00:00,  1.09s/it, loss=959662.186607]\n",
            "Epoch 2 train       : 100%|██████████| 9/9 [00:27<00:00,  3.05s/it, loss=874481.144504]\n",
            "Epoch 2 val         : 100%|██████████| 3/3 [00:03<00:00,  1.06s/it, loss=674979.547768]\n",
            "Epoch 3 train       : 100%|██████████| 9/9 [00:27<00:00,  3.01s/it, loss=681967.887411]\n",
            "Epoch 3 val         : 100%|██████████| 3/3 [00:03<00:00,  1.07s/it, loss=609155.446875]\n",
            "Epoch 4 train       : 100%|██████████| 9/9 [00:27<00:00,  3.02s/it, loss=622706.406472]\n",
            "Epoch 4 val         : 100%|██████████| 3/3 [00:03<00:00,  1.09s/it, loss=557090.110491]\n",
            "Epoch 5 train       : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=585044.291223]\n",
            "Epoch 5 val         : 100%|██████████| 3/3 [00:03<00:00,  1.08s/it, loss=532983.599777]\n",
            "Epoch 6 train       : 100%|██████████| 9/9 [00:27<00:00,  3.02s/it, loss=559575.684619]\n",
            "Epoch 6 val         : 100%|██████████| 3/3 [00:03<00:00,  1.08s/it, loss=505725.528571]\n",
            "Epoch 7 train       : 100%|██████████| 9/9 [00:27<00:00,  3.02s/it, loss=529865.230275]\n",
            "Epoch 7 val         : 100%|██████████| 3/3 [00:03<00:00,  1.07s/it, loss=472513.327679]\n",
            "Epoch 8 train       : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=488924.457225]\n",
            "Epoch 8 val         : 100%|██████████| 3/3 [00:03<00:00,  1.08s/it, loss=430163.414732]\n",
            "Epoch 9 train       : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=447654.935062]\n",
            "Epoch 9 val         : 100%|██████████| 3/3 [00:03<00:00,  1.07s/it, loss=397367.720313]\n",
            "Epoch 10 train      : 100%|██████████| 9/9 [00:27<00:00,  3.02s/it, loss=413949.461879]\n",
            "Epoch 10 val        : 100%|██████████| 3/3 [00:03<00:00,  1.08s/it, loss=366519.113616]\n",
            "Epoch 11 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=384791.232270]\n",
            "Epoch 11 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=339806.282031]\n",
            "Epoch 12 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=357797.836436]\n",
            "Epoch 12 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=317437.890067]\n",
            "Epoch 13 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=335384.791888]\n",
            "Epoch 13 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=298457.650670]\n",
            "Epoch 14 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=318107.868905]\n",
            "Epoch 14 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=283405.586830]\n",
            "Epoch 15 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=303408.133422]\n",
            "Epoch 15 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=271266.255915]\n",
            "Epoch 16 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=290137.232491]\n",
            "Epoch 16 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=261019.729799]\n",
            "Epoch 17 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=280211.663785]\n",
            "Epoch 17 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=252842.659598]\n",
            "Epoch 18 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=272232.663785]\n",
            "Epoch 18 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=245412.043973]\n",
            "Epoch 19 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=264429.909574]\n",
            "Epoch 19 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=238929.113616]\n",
            "Epoch 20 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=258206.609486]\n",
            "Epoch 20 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=233308.664174]\n",
            "Epoch 21 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=253174.780474]\n",
            "Epoch 21 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=228169.363839]\n",
            "Epoch 22 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=247454.291777]\n",
            "Epoch 22 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=223715.791853]\n",
            "Epoch 23 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=242651.202903]\n",
            "Epoch 23 val        : 100%|██████████| 3/3 [00:03<00:00,  1.09s/it, loss=219684.918862]\n",
            "Epoch 24 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=238796.668772]\n",
            "Epoch 24 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=216271.336049]\n",
            "Epoch 25 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=235434.403369]\n",
            "Epoch 25 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=213175.762500]\n",
            "Epoch 26 train      : 100%|██████████| 9/9 [00:27<00:00,  3.05s/it, loss=232009.540891]\n",
            "Epoch 26 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=210369.796987]\n",
            "Epoch 27 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=229725.033799]\n",
            "Epoch 27 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=207695.409487]\n",
            "Epoch 28 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=226943.087655]\n",
            "Epoch 28 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=205485.779353]\n",
            "Epoch 29 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=224988.003879]\n",
            "Epoch 29 val        : 100%|██████████| 3/3 [00:03<00:00,  1.12s/it, loss=203440.145982]\n",
            "Epoch 30 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=222906.124003]\n",
            "Epoch 30 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=201707.115402]\n",
            "Epoch 31 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=221404.476729]\n",
            "Epoch 31 val        : 100%|██████████| 3/3 [00:03<00:00,  1.12s/it, loss=200243.956808]\n",
            "Epoch 32 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=219514.908023]\n",
            "Epoch 32 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=198854.826339]\n",
            "Epoch 33 train      : 100%|██████████| 9/9 [00:27<00:00,  3.05s/it, loss=217817.699246]\n",
            "Epoch 33 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=197609.554799]\n",
            "Epoch 34 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=217221.821587]\n",
            "Epoch 34 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=196545.750670]\n",
            "Epoch 35 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=215607.685838]\n",
            "Epoch 35 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=195557.647656]\n",
            "Epoch 36 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=214377.961215]\n",
            "Epoch 36 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=194714.079129]\n",
            "Epoch 37 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=214217.122895]\n",
            "Epoch 37 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=193852.634821]\n",
            "Epoch 38 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=212733.683400]\n",
            "Epoch 38 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=193135.603013]\n",
            "Epoch 39 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=212523.113475]\n",
            "Epoch 39 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=192489.693973]\n",
            "Epoch 40 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=211851.692930]\n",
            "Epoch 40 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=191833.986049]\n",
            "Epoch 41 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=210672.135306]\n",
            "Epoch 41 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=191312.813616]\n",
            "Epoch 42 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=210203.669991]\n",
            "Epoch 42 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=190858.578795]\n",
            "Epoch 43 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=210068.137079]\n",
            "Epoch 43 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=190471.747768]\n",
            "Epoch 44 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=209444.278812]\n",
            "Epoch 44 val        : 100%|██████████| 3/3 [00:03<00:00,  1.09s/it, loss=190116.323437]\n",
            "Epoch 45 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=209096.509198]\n",
            "Epoch 45 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=189741.780022]\n",
            "Epoch 46 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=208329.222629]\n",
            "Epoch 46 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=189418.179911]\n",
            "Epoch 47 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=208867.610040]\n",
            "Epoch 47 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=189116.268862]\n",
            "Epoch 48 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=208635.622673]\n",
            "Epoch 48 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=188845.038951]\n",
            "Epoch 49 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=207308.981715]\n",
            "Epoch 49 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=188545.242634]\n",
            "Epoch 50 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=206850.920988]\n",
            "Epoch 50 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=188344.786496]\n",
            "Epoch 51 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=206559.982491]\n",
            "Epoch 51 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=188134.108371]\n",
            "Epoch 52 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=207202.966312]\n",
            "Epoch 52 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=187952.806250]\n",
            "Epoch 53 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=206916.724402]\n",
            "Epoch 53 val        : 100%|██████████| 3/3 [00:03<00:00,  1.09s/it, loss=187740.735045]\n",
            "Epoch 54 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=206593.870789]\n",
            "Epoch 54 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=187571.094308]\n",
            "Epoch 55 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=206825.343307]\n",
            "Epoch 55 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=187463.227009]\n",
            "Epoch 56 train      : 100%|██████████| 9/9 [00:27<00:00,  3.03s/it, loss=206384.744902]\n",
            "Epoch 56 val        : 100%|██████████| 3/3 [00:03<00:00,  1.13s/it, loss=187251.863170]\n",
            "Epoch 57 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=206139.441379]\n",
            "Epoch 57 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=187113.411161]\n",
            "Epoch 58 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=205616.254987]\n",
            "Epoch 58 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=187027.124330]\n",
            "Epoch 59 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=205958.350842]\n",
            "Epoch 59 val        : 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, loss=186867.691908]\n",
            "Epoch 60 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=206272.344082]\n",
            "Epoch 60 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=186804.755357]\n",
            "Epoch 61 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=205736.518728]\n",
            "Epoch 61 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=186709.701674]\n",
            "Epoch 62 train      : 100%|██████████| 9/9 [00:27<00:00,  3.04s/it, loss=205789.048316]\n",
            "Epoch 62 val        : 100%|██████████| 3/3 [00:03<00:00,  1.14s/it, loss=186604.583259]\n",
            "Epoch 63 train      : 100%|██████████| 9/9 [00:27<00:00,  3.05s/it, loss=205413.507979]\n",
            "Epoch 63 val        : 100%|██████████| 3/3 [00:03<00:00,  1.10s/it, loss=186487.286049]\n",
            "Epoch 64 train      :   0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkYBl9H52A9p"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OtoWy6n5L9i"
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAyXLc5s7zlM"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/0.93_100.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}