{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "together_dropout_linear_width_inc.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/together_dropout_linear_width_inc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "outputId": "65a1616e-3321-49a0-a152-624a90e82dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/gdrive/My Drive/lungs/train.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "outputId": "b1f58ea0-f7ed-46b3-b39f-6ca14fc5f90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "outputId": "c681bfdc-82c6-4120-89a1-adf8406eb473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "\n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "176it [00:00, 718.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self,TAB,A, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "        self.TAB=TAB\n",
        "        self.A=A\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image,\n",
        "            'tab':self.TAB[self.tensor_files[item].stem],\n",
        "            'slope':self.A[self.tensor_files[item].stem]\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGULd4YJatQz"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 10\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "# cum = 0\n",
        "# for i in range(len(train)):\n",
        "#     sample = train[i]['image']\n",
        "#     cum += torch.mean(sample).item()\n",
        "\n",
        "# assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "\n",
        "        #tabular\n",
        "        self.tab1=nn.Linear(104,512)\n",
        "        self.tab2=nn.Linear(512,512)\n",
        "        self.tab3=nn.Linear(512,100)\n",
        "\n",
        "\n",
        "        #output\n",
        "        self.out1=nn.Linear(100,512)\n",
        "        self.out2=nn.Linear(512,512)\n",
        "        self.out3=nn.Linear(512,1)\n",
        "\n",
        "\n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x,y, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        x = torch.cat((x,y),1)\n",
        "        x = self.act(self.tab1(x))\n",
        "        x = self.act(self.tab2(x))\n",
        "        x = self.tab3(x)\n",
        "\n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4,x\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x,y):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4,out = self.encode(x,y)\n",
        "        \n",
        "        out1=self.act(self.out1(out))\n",
        "        out2=self.act(self.out2(out1))\n",
        "        output=self.out3(out2)\n",
        "\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var,output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv",
        "outputId": "b7be9b48-cbec-4ae9-8a5d-ed27faea2916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(300):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            tabular=inputs['tab'].to(device, dtype=torch.float)\n",
        "            slopes_input=inputs['slope'].to(device, dtype=torch.float)\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var,slope_output = model(inputs,tabular)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                slope_loss = F.mse_loss(slope_output,slopes_input)\n",
        "                \n",
        "                loss =  reconst_loss + kl_div+slope_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 train       :   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       :  89%|████████▉ | 8/9 [00:28<00:03,  3.49s/it, loss=581138.812500]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       : 100%|██████████| 9/9 [00:31<00:00,  3.53s/it, loss=563828.851507]\n",
            "Epoch 1 val         :  67%|██████▋   | 2/3 [00:01<00:01,  1.05s/it, loss=560932.062500]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 val         : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=522601.028348]\n",
            "Epoch 2 train       : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=539447.665780]\n",
            "Epoch 2 val         : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=491974.278348]\n",
            "Epoch 3 train       : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=479148.573582]\n",
            "Epoch 3 val         : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=422818.502232]\n",
            "Epoch 4 train       : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=409143.190824]\n",
            "Epoch 4 val         : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=348827.527902]\n",
            "Epoch 5 train       : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=340064.783466]\n",
            "Epoch 5 val         : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=304337.588839]\n",
            "Epoch 6 train       : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=301568.104167]\n",
            "Epoch 6 val         : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=267693.408594]\n",
            "Epoch 7 train       : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=262764.723293]\n",
            "Epoch 7 val         : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=234516.715513]\n",
            "Epoch 8 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=233246.822141]\n",
            "Epoch 8 val         : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=212789.152344]\n",
            "Epoch 9 train       : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=212636.658688]\n",
            "Epoch 9 val         : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=196652.420089]\n",
            "Epoch 10 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=197045.153812]\n",
            "Epoch 10 val        : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=183888.177679]\n",
            "Epoch 11 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=185984.653147]\n",
            "Epoch 11 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=179208.183036]\n",
            "Epoch 12 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=181273.714207]\n",
            "Epoch 12 val        : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=173595.910714]\n",
            "Epoch 13 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=175120.234818]\n",
            "Epoch 13 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=167147.819978]\n",
            "Epoch 14 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=168317.109929]\n",
            "Epoch 14 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=161176.396094]\n",
            "Epoch 15 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=162676.362589]\n",
            "Epoch 15 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=157168.795759]\n",
            "Epoch 16 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=158711.697584]\n",
            "Epoch 16 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=153129.562054]\n",
            "Epoch 17 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=154643.009530]\n",
            "Epoch 17 val        : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=149255.409040]\n",
            "Epoch 18 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=150357.127438]\n",
            "Epoch 18 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=145439.722321]\n",
            "Epoch 19 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=146484.408134]\n",
            "Epoch 19 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=141991.568806]\n",
            "Epoch 20 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=143819.695368]\n",
            "Epoch 20 val        : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=140288.596038]\n",
            "Epoch 21 train      : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=141377.311613]\n",
            "Epoch 21 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=137210.766629]\n",
            "Epoch 22 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=138731.081671]\n",
            "Epoch 22 val        : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=134869.972768]\n",
            "Epoch 23 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=136568.767952]\n",
            "Epoch 23 val        : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=133299.032868]\n",
            "Epoch 24 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=134379.908078]\n",
            "Epoch 24 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=131939.820647]\n",
            "Epoch 25 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=133478.762467]\n",
            "Epoch 25 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=130614.681417]\n",
            "Epoch 26 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=132229.411181]\n",
            "Epoch 26 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=129962.985882]\n",
            "Epoch 27 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=131331.575798]\n",
            "Epoch 27 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=128962.722545]\n",
            "Epoch 28 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129998.149047]\n",
            "Epoch 28 val        : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=127839.763170]\n",
            "Epoch 29 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=128994.400377]\n",
            "Epoch 29 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=126457.715179]\n",
            "Epoch 30 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=127844.188165]\n",
            "Epoch 30 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=125705.393750]\n",
            "Epoch 31 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=126696.543661]\n",
            "Epoch 31 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=124641.809487]\n",
            "Epoch 32 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=125620.596243]\n",
            "Epoch 32 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=123784.206306]\n",
            "Epoch 33 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=124912.336436]\n",
            "Epoch 33 val        : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=123656.820759]\n",
            "Epoch 34 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124526.868074]\n",
            "Epoch 34 val        : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=122954.472935]\n",
            "Epoch 35 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=124111.383533]\n",
            "Epoch 35 val        : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=122656.107813]\n",
            "Epoch 36 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123280.015348]\n",
            "Epoch 36 val        : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=121730.087667]\n",
            "Epoch 37 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122484.746121]\n",
            "Epoch 37 val        : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=121146.610882]\n",
            "Epoch 38 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121782.577903]\n",
            "Epoch 38 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=120411.711217]\n",
            "Epoch 39 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121427.232824]\n",
            "Epoch 39 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=119997.694866]\n",
            "Epoch 40 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=120760.916999]\n",
            "Epoch 40 val        : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=119560.305022]\n",
            "Epoch 41 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=120304.391512]\n",
            "Epoch 41 val        : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=119034.942355]\n",
            "Epoch 42 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=119738.703014]\n",
            "Epoch 42 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=118567.703348]\n",
            "Epoch 43 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=119305.546210]\n",
            "Epoch 43 val        : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=118235.837779]\n",
            "Epoch 44 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=118852.147052]\n",
            "Epoch 44 val        : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=117826.085491]\n",
            "Epoch 45 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=118611.026263]\n",
            "Epoch 45 val        : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=117537.158984]\n",
            "Epoch 46 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=118348.038010]\n",
            "Epoch 46 val        : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=117315.165960]\n",
            "Epoch 47 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=117980.742631]\n",
            "Epoch 47 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=116819.774609]\n",
            "Epoch 48 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=117365.594415]\n",
            "Epoch 48 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=116469.811272]\n",
            "Epoch 49 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=117194.403147]\n",
            "Epoch 49 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=116238.819810]\n",
            "Epoch 50 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=116806.668772]\n",
            "Epoch 50 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=115997.369141]\n",
            "Epoch 51 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=116468.940769]\n",
            "Epoch 51 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=115767.117634]\n",
            "Epoch 52 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=116516.646720]\n",
            "Epoch 52 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=115575.597266]\n",
            "Epoch 53 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=115855.463763]\n",
            "Epoch 53 val        : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=115401.163616]\n",
            "Epoch 54 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=116163.518506]\n",
            "Epoch 54 val        : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=115091.211384]\n",
            "Epoch 55 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=115550.796432]\n",
            "Epoch 55 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=114812.974665]\n",
            "Epoch 56 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=115349.336104]\n",
            "Epoch 56 val        : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=114733.708036]\n",
            "Epoch 57 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=115182.806793]\n",
            "Epoch 57 val        : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=114591.254855]\n",
            "Epoch 58 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=115123.314162]\n",
            "Epoch 58 val        : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=114314.654464]\n",
            "Epoch 59 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=114731.693262]\n",
            "Epoch 59 val        : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=114158.941853]\n",
            "Epoch 60 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=114648.559785]\n",
            "Epoch 60 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=113945.053292]\n",
            "Epoch 61 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=114108.469082]\n",
            "Epoch 61 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=113713.029185]\n",
            "Epoch 62 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=114018.943262]\n",
            "Epoch 62 val        : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=113626.189397]\n",
            "Epoch 63 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=113664.629322]\n",
            "Epoch 63 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=113464.346484]\n",
            "Epoch 64 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=113683.493296]\n",
            "Epoch 64 val        : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=113369.636886]\n",
            "Epoch 65 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=113627.497340]\n",
            "Epoch 65 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=113281.258705]\n",
            "Epoch 66 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=113369.518285]\n",
            "Epoch 66 val        : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113102.167969]\n",
            "Epoch 67 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=113552.927139]\n",
            "Epoch 67 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113061.366574]\n",
            "Epoch 68 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=113364.554355]\n",
            "Epoch 68 val        : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=112852.817243]\n",
            "Epoch 69 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=113268.123559]\n",
            "Epoch 69 val        : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=112828.322712]\n",
            "Epoch 70 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=112949.626884]\n",
            "Epoch 70 val        : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=112632.026339]\n",
            "Epoch 71 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=112964.017232]\n",
            "Epoch 71 val        : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=112604.994252]\n",
            "Epoch 72 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=112940.859763]\n",
            "Epoch 72 val        : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=112469.492243]\n",
            "Epoch 73 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=112830.974900]\n",
            "Epoch 73 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=112348.891462]\n",
            "Epoch 74 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112723.963431]\n",
            "Epoch 74 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=112278.427846]\n",
            "Epoch 75 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112460.424479]\n",
            "Epoch 75 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=112170.994475]\n",
            "Epoch 76 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112407.839761]\n",
            "Epoch 76 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=112155.456585]\n",
            "Epoch 77 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112407.567542]\n",
            "Epoch 77 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=112067.531027]\n",
            "Epoch 78 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=112274.250887]\n",
            "Epoch 78 val        : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=111956.498270]\n",
            "Epoch 79 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112236.455064]\n",
            "Epoch 79 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=111842.494699]\n",
            "Epoch 80 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=112072.137023]\n",
            "Epoch 80 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=111723.663951]\n",
            "Epoch 81 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111936.350621]\n",
            "Epoch 81 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=111718.013449]\n",
            "Epoch 82 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=111794.874391]\n",
            "Epoch 82 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=111607.722489]\n",
            "Epoch 83 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=111873.220855]\n",
            "Epoch 83 val        : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=111586.900614]\n",
            "Epoch 84 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=112127.064439]\n",
            "Epoch 84 val        : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=111556.015290]\n",
            "Epoch 85 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=111889.603779]\n",
            "Epoch 85 val        : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=111409.602009]\n",
            "Epoch 86 train      : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=111638.428247]\n",
            "Epoch 86 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=111348.960491]\n",
            "Epoch 87 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=111551.611979]\n",
            "Epoch 87 val        : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=111329.720592]\n",
            "Epoch 88 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111470.683511]\n",
            "Epoch 88 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=111301.869141]\n",
            "Epoch 89 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111506.645889]\n",
            "Epoch 89 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=111253.183873]\n",
            "Epoch 90 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111609.632369]\n",
            "Epoch 90 val        : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=111200.967299]\n",
            "Epoch 91 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111218.685616]\n",
            "Epoch 91 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=111201.538783]\n",
            "Epoch 92 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111193.913453]\n",
            "Epoch 92 val        : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=111112.411998]\n",
            "Epoch 93 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111257.133533]\n",
            "Epoch 93 val        : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=111137.239007]\n",
            "Epoch 94 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111373.496676]\n",
            "Epoch 94 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=111051.639900]\n",
            "Epoch 95 train      : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=111242.769005]\n",
            "Epoch 95 val        : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110953.829743]\n",
            "Epoch 96 train      : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=111193.991633]\n",
            "Epoch 96 val        : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110937.757757]\n",
            "Epoch 97 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=111251.969692]\n",
            "Epoch 97 val        : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110920.790458]\n",
            "Epoch 98 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=111059.292332]\n",
            "Epoch 98 val        : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110944.958761]\n",
            "Epoch 99 train      : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=111118.145445]\n",
            "Epoch 99 val        : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110822.811440]\n",
            "Epoch 100 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110978.042332]\n",
            "Epoch 100 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110783.278906]\n",
            "Epoch 101 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110939.451352]\n",
            "Epoch 101 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110767.562723]\n",
            "Epoch 102 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=111026.715980]\n",
            "Epoch 102 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110709.901507]\n",
            "Epoch 103 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110859.187555]\n",
            "Epoch 103 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110719.950167]\n",
            "Epoch 104 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110887.657691]\n",
            "Epoch 104 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110705.301339]\n",
            "Epoch 105 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110883.162511]\n",
            "Epoch 105 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110705.178237]\n",
            "Epoch 106 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110894.438719]\n",
            "Epoch 106 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110612.745368]\n",
            "Epoch 107 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110889.350288]\n",
            "Epoch 107 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110627.846763]\n",
            "Epoch 108 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110855.496676]\n",
            "Epoch 108 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110555.942411]\n",
            "Epoch 109 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110783.194149]\n",
            "Epoch 109 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110516.731027]\n",
            "Epoch 110 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110651.371842]\n",
            "Epoch 110 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110555.242969]\n",
            "Epoch 111 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110883.121953]\n",
            "Epoch 111 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=110503.759375]\n",
            "Epoch 112 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110688.302693]\n",
            "Epoch 112 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110529.897042]\n",
            "Epoch 113 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110931.922595]\n",
            "Epoch 113 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110427.260658]\n",
            "Epoch 114 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110849.252937]\n",
            "Epoch 114 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110438.446429]\n",
            "Epoch 115 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110734.489694]\n",
            "Epoch 115 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110455.528850]\n",
            "Epoch 116 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110510.013741]\n",
            "Epoch 116 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110406.949721]\n",
            "Epoch 117 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110965.478502]\n",
            "Epoch 117 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110396.637500]\n",
            "Epoch 118 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110679.881704]\n",
            "Epoch 118 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110366.246094]\n",
            "Epoch 119 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110566.621066]\n",
            "Epoch 119 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110360.458036]\n",
            "Epoch 120 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110318.112533]\n",
            "Epoch 120 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110327.348828]\n",
            "Epoch 121 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110660.197584]\n",
            "Epoch 121 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110353.407366]\n",
            "Epoch 122 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110726.376385]\n",
            "Epoch 122 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110356.211272]\n",
            "Epoch 123 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110503.138242]\n",
            "Epoch 123 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110332.183092]\n",
            "Epoch 124 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110301.503602]\n",
            "Epoch 124 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110306.090123]\n",
            "Epoch 125 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110610.388797]\n",
            "Epoch 125 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=110345.999442]\n",
            "Epoch 126 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110476.257314]\n",
            "Epoch 126 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110282.238337]\n",
            "Epoch 127 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110507.598016]\n",
            "Epoch 127 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110236.592355]\n",
            "Epoch 128 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110480.103280]\n",
            "Epoch 128 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110257.523549]\n",
            "Epoch 129 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110504.440769]\n",
            "Epoch 129 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110239.311384]\n",
            "Epoch 130 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110315.192487]\n",
            "Epoch 130 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110193.152121]\n",
            "Epoch 131 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110320.496398]\n",
            "Epoch 131 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110255.613002]\n",
            "Epoch 132 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110613.262356]\n",
            "Epoch 132 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110198.079408]\n",
            "Epoch 133 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110230.033688]\n",
            "Epoch 133 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110173.373047]\n",
            "Epoch 134 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110351.831062]\n",
            "Epoch 134 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110215.025391]\n",
            "Epoch 135 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110246.842808]\n",
            "Epoch 135 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110166.323717]\n",
            "Epoch 136 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110226.714539]\n",
            "Epoch 136 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110206.717076]\n",
            "Epoch 137 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110476.407247]\n",
            "Epoch 137 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110190.478181]\n",
            "Epoch 138 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110244.188553]\n",
            "Epoch 138 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110167.000335]\n",
            "Epoch 139 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110409.074413]\n",
            "Epoch 139 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110142.898996]\n",
            "Epoch 140 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110162.734984]\n",
            "Epoch 140 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110175.052009]\n",
            "Epoch 141 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110165.760029]\n",
            "Epoch 141 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110177.291629]\n",
            "Epoch 142 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110372.438220]\n",
            "Epoch 142 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110151.257478]\n",
            "Epoch 143 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110368.960217]\n",
            "Epoch 143 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110155.336942]\n",
            "Epoch 144 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110293.507037]\n",
            "Epoch 144 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110111.599219]\n",
            "Epoch 145 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110344.188885]\n",
            "Epoch 145 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110128.686440]\n",
            "Epoch 146 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110233.088819]\n",
            "Epoch 146 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110076.902902]\n",
            "Epoch 147 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110266.806128]\n",
            "Epoch 147 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=110090.237556]\n",
            "Epoch 148 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110094.175864]\n",
            "Epoch 148 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110083.927232]\n",
            "Epoch 149 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110494.082835]\n",
            "Epoch 149 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110099.585547]\n",
            "Epoch 150 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110146.676862]\n",
            "Epoch 150 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=110087.887388]\n",
            "Epoch 151 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110166.212156]\n",
            "Epoch 151 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=110084.727902]\n",
            "Epoch 152 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110003.578624]\n",
            "Epoch 152 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110111.737054]\n",
            "Epoch 153 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110258.529699]\n",
            "Epoch 153 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110040.573047]\n",
            "Epoch 154 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110216.277482]\n",
            "Epoch 154 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110099.791741]\n",
            "Epoch 155 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110019.132148]\n",
            "Epoch 155 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=110110.476842]\n",
            "Epoch 156 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110194.098072]\n",
            "Epoch 156 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110103.410324]\n",
            "Epoch 157 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110260.019947]\n",
            "Epoch 157 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110067.028460]\n",
            "Epoch 158 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110318.995124]\n",
            "Epoch 158 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110047.868415]\n",
            "Epoch 159 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110466.798759]\n",
            "Epoch 159 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110074.204687]\n",
            "Epoch 160 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110356.704843]\n",
            "Epoch 160 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110007.849051]\n",
            "Epoch 161 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110270.543440]\n",
            "Epoch 161 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110039.409654]\n",
            "Epoch 162 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110233.492021]\n",
            "Epoch 162 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=110056.631585]\n",
            "Epoch 163 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110222.262965]\n",
            "Epoch 163 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110069.857254]\n",
            "Epoch 164 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110308.143451]\n",
            "Epoch 164 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109977.228292]\n",
            "Epoch 165 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110003.185284]\n",
            "Epoch 165 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110061.339509]\n",
            "Epoch 166 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110317.476895]\n",
            "Epoch 166 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110021.981920]\n",
            "Epoch 167 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110194.065935]\n",
            "Epoch 167 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110054.346094]\n",
            "Epoch 168 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110361.245013]\n",
            "Epoch 168 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110033.871931]\n",
            "Epoch 169 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110196.774767]\n",
            "Epoch 169 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110037.986607]\n",
            "Epoch 170 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110161.402482]\n",
            "Epoch 170 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110019.484431]\n",
            "Epoch 171 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110313.796321]\n",
            "Epoch 171 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110025.469252]\n",
            "Epoch 172 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110028.397274]\n",
            "Epoch 172 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110051.686663]\n",
            "Epoch 173 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110170.844138]\n",
            "Epoch 173 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110022.887054]\n",
            "Epoch 174 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110186.428967]\n",
            "Epoch 174 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110061.125670]\n",
            "Epoch 175 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=109985.647329]\n",
            "Epoch 175 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110021.292467]\n",
            "Epoch 176 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110172.498449]\n",
            "Epoch 176 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110030.232310]\n",
            "Epoch 177 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110364.345357]\n",
            "Epoch 177 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110031.882812]\n",
            "Epoch 178 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110144.916002]\n",
            "Epoch 178 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110041.030804]\n",
            "Epoch 179 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110077.059951]\n",
            "Epoch 179 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110021.305692]\n",
            "Epoch 180 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110261.335439]\n",
            "Epoch 180 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110009.067801]\n",
            "Epoch 181 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110155.532414]\n",
            "Epoch 181 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110049.197321]\n",
            "Epoch 182 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110170.801695]\n",
            "Epoch 182 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109980.947768]\n",
            "Epoch 183 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110269.153590]\n",
            "Epoch 183 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110017.521205]\n",
            "Epoch 184 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110267.963819]\n",
            "Epoch 184 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=110003.449442]\n",
            "Epoch 185 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110482.413453]\n",
            "Epoch 185 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110014.995815]\n",
            "Epoch 186 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110274.272773]\n",
            "Epoch 186 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109990.278404]\n",
            "Epoch 187 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110509.645833]\n",
            "Epoch 187 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110045.822489]\n",
            "Epoch 188 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110200.266622]\n",
            "Epoch 188 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110015.651786]\n",
            "Epoch 189 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110154.662345]\n",
            "Epoch 189 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109962.078906]\n",
            "Epoch 190 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=109915.717143]\n",
            "Epoch 190 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110024.908984]\n",
            "Epoch 191 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110434.965703]\n",
            "Epoch 191 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=109977.038560]\n",
            "Epoch 192 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110038.596133]\n",
            "Epoch 192 val       : 100%|██████████| 3/3 [00:02<00:00,  1.41it/s, loss=109971.397098]\n",
            "Epoch 193 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=109890.547484]\n",
            "Epoch 193 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110024.371763]\n",
            "Epoch 194 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110332.341146]\n",
            "Epoch 194 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109992.742299]\n",
            "Epoch 195 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110137.316046]\n",
            "Epoch 195 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110050.856027]\n",
            "Epoch 196 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110050.275155]\n",
            "Epoch 196 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109995.260938]\n",
            "Epoch 197 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110008.195534]\n",
            "Epoch 197 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=109978.383147]\n",
            "Epoch 198 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110188.722074]\n",
            "Epoch 198 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110025.052176]\n",
            "Epoch 199 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110110.590813]\n",
            "Epoch 199 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110027.100558]\n",
            "Epoch 200 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110257.081339]\n",
            "Epoch 200 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110009.730469]\n",
            "Epoch 201 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110200.944260]\n",
            "Epoch 201 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110015.705525]\n",
            "Epoch 202 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110126.943373]\n",
            "Epoch 202 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109954.751228]\n",
            "Epoch 203 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=109828.090038]\n",
            "Epoch 203 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110053.410937]\n",
            "Epoch 204 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110081.252881]\n",
            "Epoch 204 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109993.110770]\n",
            "Epoch 205 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110375.352283]\n",
            "Epoch 205 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109977.441406]\n",
            "Epoch 206 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110086.183621]\n",
            "Epoch 206 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109975.248214]\n",
            "Epoch 207 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110212.681461]\n",
            "Epoch 207 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109964.470368]\n",
            "Epoch 208 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110253.961492]\n",
            "Epoch 208 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110047.421038]\n",
            "Epoch 209 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110209.396831]\n",
            "Epoch 209 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110012.735993]\n",
            "Epoch 210 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110158.567875]\n",
            "Epoch 210 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110013.603460]\n",
            "Epoch 211 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110101.872895]\n",
            "Epoch 211 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110013.214844]\n",
            "Epoch 212 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110275.161680]\n",
            "Epoch 212 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=109981.620536]\n",
            "Epoch 213 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110163.423593]\n",
            "Epoch 213 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=109986.362556]\n",
            "Epoch 214 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110206.956893]\n",
            "Epoch 214 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110031.077176]\n",
            "Epoch 215 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110003.023659]\n",
            "Epoch 215 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109986.522098]\n",
            "Epoch 216 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110082.592808]\n",
            "Epoch 216 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109980.194978]\n",
            "Epoch 217 train     : 100%|██████████| 9/9 [00:31<00:00,  3.49s/it, loss=109931.131316]\n",
            "Epoch 217 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109977.270089]\n",
            "Epoch 218 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110207.342808]\n",
            "Epoch 218 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110008.937556]\n",
            "Epoch 219 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110134.111037]\n",
            "Epoch 219 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110010.211607]\n",
            "Epoch 220 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109900.377272]\n",
            "Epoch 220 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110050.145257]\n",
            "Epoch 221 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110070.595855]\n",
            "Epoch 221 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109988.504911]\n",
            "Epoch 222 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110399.644504]\n",
            "Epoch 222 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=109980.208426]\n",
            "Epoch 223 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110094.724346]\n",
            "Epoch 223 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=109984.551953]\n",
            "Epoch 224 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110196.267675]\n",
            "Epoch 224 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110006.467243]\n",
            "Epoch 225 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110352.761636]\n",
            "Epoch 225 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110017.170759]\n",
            "Epoch 226 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110130.639794]\n",
            "Epoch 226 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109958.416853]\n",
            "Epoch 227 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110118.083333]\n",
            "Epoch 227 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110030.073661]\n",
            "Epoch 228 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109987.100399]\n",
            "Epoch 228 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110030.617076]\n",
            "Epoch 229 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109965.816877]\n",
            "Epoch 229 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=110011.767299]\n",
            "Epoch 230 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=109956.307735]\n",
            "Epoch 230 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109924.629967]\n",
            "Epoch 231 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110390.798648]\n",
            "Epoch 231 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=109987.047489]\n",
            "Epoch 232 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110330.606272]\n",
            "Epoch 232 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=109979.953627]\n",
            "Epoch 233 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110165.747063]\n",
            "Epoch 233 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110009.856529]\n",
            "Epoch 234 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110054.602671]\n",
            "Epoch 234 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110010.173605]\n",
            "Epoch 235 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110197.555408]\n",
            "Epoch 235 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109993.960547]\n",
            "Epoch 236 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110394.156028]\n",
            "Epoch 236 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110004.017578]\n",
            "Epoch 237 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110093.527593]\n",
            "Epoch 237 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=109996.708203]\n",
            "Epoch 238 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=109982.756760]\n",
            "Epoch 238 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110007.597321]\n",
            "Epoch 239 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110005.829289]\n",
            "Epoch 239 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109931.511998]\n",
            "Epoch 240 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110105.153867]\n",
            "Epoch 240 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=109994.673884]\n",
            "Epoch 241 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109988.034519]\n",
            "Epoch 241 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109950.828181]\n",
            "Epoch 242 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109948.131594]\n",
            "Epoch 242 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109956.875000]\n",
            "Epoch 243 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110049.699634]\n",
            "Epoch 243 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109966.265346]\n",
            "Epoch 244 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110029.488641]\n",
            "Epoch 244 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109982.558984]\n",
            "Epoch 245 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110106.493240]\n",
            "Epoch 245 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109970.779855]\n",
            "Epoch 246 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110356.034464]\n",
            "Epoch 246 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109993.879687]\n",
            "Epoch 247 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110265.514240]\n",
            "Epoch 247 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109993.037556]\n",
            "Epoch 248 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110268.234264]\n",
            "Epoch 248 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110012.491350]\n",
            "Epoch 249 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110336.625554]\n",
            "Epoch 249 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110013.269978]\n",
            "Epoch 250 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110051.709829]\n",
            "Epoch 250 val       : 100%|██████████| 3/3 [00:02<00:00,  1.42it/s, loss=110033.035100]\n",
            "Epoch 251 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109673.046487]\n",
            "Epoch 251 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110020.968471]\n",
            "Epoch 252 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110272.547872]\n",
            "Epoch 252 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=109993.075614]\n",
            "Epoch 253 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110026.551585]\n",
            "Epoch 253 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109969.409989]\n",
            "Epoch 254 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110176.803081]\n",
            "Epoch 254 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110009.325167]\n",
            "Epoch 255 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110342.732657]\n",
            "Epoch 255 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109951.753962]\n",
            "Epoch 256 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110157.243739]\n",
            "Epoch 256 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=110011.481194]\n",
            "Epoch 257 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110208.083112]\n",
            "Epoch 257 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=110008.912388]\n",
            "Epoch 258 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110179.771997]\n",
            "Epoch 258 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109990.500670]\n",
            "Epoch 259 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110129.072363]\n",
            "Epoch 259 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109995.912612]\n",
            "Epoch 260 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110175.357768]\n",
            "Epoch 260 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110010.323884]\n",
            "Epoch 261 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110171.743517]\n",
            "Epoch 261 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110001.235937]\n",
            "Epoch 262 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110093.169326]\n",
            "Epoch 262 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109978.469531]\n",
            "Epoch 263 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110069.646886]\n",
            "Epoch 263 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109982.328460]\n",
            "Epoch 264 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110452.642952]\n",
            "Epoch 264 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109975.621708]\n",
            "Epoch 265 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=109749.263741]\n",
            "Epoch 265 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109954.698493]\n",
            "Epoch 266 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110279.300310]\n",
            "Epoch 266 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=109986.284208]\n",
            "Epoch 267 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110221.921764]\n",
            "Epoch 267 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110049.894643]\n",
            "Epoch 268 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110213.934619]\n",
            "Epoch 268 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109922.000502]\n",
            "Epoch 269 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110235.287456]\n",
            "Epoch 269 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109990.769029]\n",
            "Epoch 270 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110058.216977]\n",
            "Epoch 270 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109963.351674]\n",
            "Epoch 271 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110237.255541]\n",
            "Epoch 271 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109981.458092]\n",
            "Epoch 272 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=109779.932901]\n",
            "Epoch 272 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=109987.849888]\n",
            "Epoch 273 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110240.122396]\n",
            "Epoch 273 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109990.976451]\n",
            "Epoch 274 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109935.435284]\n",
            "Epoch 274 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110014.721317]\n",
            "Epoch 275 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110410.687666]\n",
            "Epoch 275 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110036.573270]\n",
            "Epoch 276 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110033.998781]\n",
            "Epoch 276 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110009.229018]\n",
            "Epoch 277 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=110145.028480]\n",
            "Epoch 277 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=110005.228013]\n",
            "Epoch 278 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110299.773604]\n",
            "Epoch 278 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=109996.606250]\n",
            "Epoch 279 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110057.622119]\n",
            "Epoch 279 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109975.715458]\n",
            "Epoch 280 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110281.343030]\n",
            "Epoch 280 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110012.681083]\n",
            "Epoch 281 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110057.819537]\n",
            "Epoch 281 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110005.468415]\n",
            "Epoch 282 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110064.924424]\n",
            "Epoch 282 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109998.706362]\n",
            "Epoch 283 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110135.518285]\n",
            "Epoch 283 val       : 100%|██████████| 3/3 [00:02<00:00,  1.40it/s, loss=109980.125056]\n",
            "Epoch 284 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110029.095689]\n",
            "Epoch 284 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110004.185603]\n",
            "Epoch 285 train     : 100%|██████████| 9/9 [00:31<00:00,  3.48s/it, loss=110154.843584]\n",
            "Epoch 285 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109958.613951]\n",
            "Epoch 286 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110063.740636]\n",
            "Epoch 286 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=110006.663449]\n",
            "Epoch 287 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110102.656582]\n",
            "Epoch 287 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109993.710100]\n",
            "Epoch 288 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109955.360151]\n",
            "Epoch 288 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=110000.687556]\n",
            "Epoch 289 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=109977.725344]\n",
            "Epoch 289 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=109993.160714]\n",
            "Epoch 290 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=109900.192320]\n",
            "Epoch 290 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=110000.797266]\n",
            "Epoch 291 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110163.995567]\n",
            "Epoch 291 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=109987.541964]\n",
            "Epoch 292 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110129.045379]\n",
            "Epoch 292 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109964.593862]\n",
            "Epoch 293 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110206.665226]\n",
            "Epoch 293 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=109987.838281]\n",
            "Epoch 294 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110035.364583]\n",
            "Epoch 294 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109956.056027]\n",
            "Epoch 295 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110396.544105]\n",
            "Epoch 295 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109997.840067]\n",
            "Epoch 296 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110125.970689]\n",
            "Epoch 296 val       : 100%|██████████| 3/3 [00:02<00:00,  1.38it/s, loss=109925.451674]\n",
            "Epoch 297 train     : 100%|██████████| 9/9 [00:31<00:00,  3.49s/it, loss=110079.916500]\n",
            "Epoch 297 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=109943.919699]\n",
            "Epoch 298 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110101.795102]\n",
            "Epoch 298 val       : 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, loss=110018.164062]\n",
            "Epoch 299 train     : 100%|██████████| 9/9 [00:31<00:00,  3.47s/it, loss=110036.754987]\n",
            "Epoch 299 val       : 100%|██████████| 3/3 [00:02<00:00,  1.44it/s, loss=110003.722321]\n",
            "Epoch 300 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=110183.207668]\n",
            "Epoch 300 val       : 100%|██████████| 3/3 [00:02<00:00,  1.43it/s, loss=109969.178460]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done! Time 2:46:28.972249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBMMJ3oMcLGH",
        "outputId": "facd6ce6-3758-41e2-8c93-448ecfbf06fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c01aeafd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcgaYQQ7cLDR",
        "outputId": "32d4516b-de16-453b-8d47-f9693a9acfe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c01298630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvZEWNQUcLAl"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/0.95_100.pt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UefI1PrrcK9d"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdMD3yGcK6g"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}