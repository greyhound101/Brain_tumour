{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_loss_variational_autoencoder.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/0_loss_variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "outputId": "3981c8a5-3303-4a67-def0-b200a71f16dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import copy\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "from torchvision import transforms\n",
        "import warnings\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "import glob\n",
        "image_paths=glob.glob('/content/gdrive/My Drive/ID*_segmented')\n",
        "ids=[i.split('/')[-1].split('_')[0] for i in image_paths]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blnEh_PMdZuc"
      },
      "source": [
        "ls=[]\n",
        "for i in image_paths:\n",
        "  ls.append(np.load(i+'/image.npy').reshape((40,256,256,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "outputId": "a10bb05f-0abd-4915-e0e8-bf0b44eb0a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/gdrive/My Drive/lungs/train.csv') \n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(ids)):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "174it [00:00, 908.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "class IGenerator(Sequence):\n",
        "    def __init__(self, ids,image_paths, batch_size=32):\n",
        "        self.keys = ids\n",
        "        self.batch_size = batch_size\n",
        "        self.image_paths=image_paths\n",
        "        self.train_data = {}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return 2000\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        self.x= []\n",
        "        a, tab = [], [] \n",
        "        keys = np.random.choice(self.keys, size = self.batch_size)\n",
        "        for k in keys:\n",
        "            #try:\n",
        "                chosen_idx=self.keys.index(k)\n",
        "                chosen_path=self.image_paths[chosen_idx]\n",
        "                img=chosen_path\n",
        "                self.x.append(img)\n",
        "        del img\n",
        "        gc.collect()\n",
        "        self.x = np.array(self.x)\n",
        "        # self.x = np.expand_dims(self.x, axis=-1)\n",
        "        return self.x , None\n",
        "    def on_batch_end(self):\n",
        "      del self.x\n",
        "      gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "train=IGenerator(ids,ls,1)\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "def build_model(shape=(40,256,256, 1), model_class=None,latent_dim=2):\n",
        "  input_img = Input(shape=shape)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #VAE\n",
        "    \n",
        "\n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu')(input_img)\n",
        "  x = layers.MaxPooling3D()(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu',\n",
        "                  )(x)\n",
        "  x = layers.MaxPooling3D()(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu')(x)\n",
        "  x = layers.MaxPooling3D()(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  # # need to know the shape of the network here for the decoder\n",
        "  # shape_before_flattening = K.int_shape(x)\n",
        "  shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(32, activation='relu')(x)\n",
        "\n",
        "\n",
        "  # inp2 = Input(shape=(4,))\n",
        "  # x1 = Concatenate()([inp2,x]) \n",
        "  # x1 = Dense(256,activation='relu')(x1)\n",
        "  # x1 = BatchNormalization()(x1)\n",
        "  # x1 = Dropout(0.3)(x1) \n",
        "  # x1 = Dense(256,activation='relu')(x1)\n",
        "  # x1 = BatchNormalization()(x1)\n",
        "  # x1 = Dropout(0.3)(x1) \n",
        "  # x1 = Dense(256,activation='relu')(x1)\n",
        "  # x1 = BatchNormalization()(x1)\n",
        "  # x1 = Dropout(0.3)(x1) \n",
        "  # x1 = Dense(1)(x1)\n",
        "  # Two outputs, latent mean and (log)variance\n",
        "  z_mu = layers.Dense(latent_dim)(x)\n",
        "  z_log_sigma = layers.Dense(latent_dim)(x)\n",
        "\n",
        "  # #sampling\n",
        "  def sampling(args):\n",
        "    z_mu, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n",
        "                              mean=0., stddev=1.)\n",
        "    return z_mu + K.exp(z_log_sigma) * epsilon\n",
        "\n",
        "  # sample vector from the latent distribution\n",
        "  z = layers.Lambda(sampling)([z_mu, z_log_sigma])\n",
        "  # construct a custom layer to calculate the loss\n",
        "  # decoder takes the latent distribution sample as input\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #decoder\n",
        "  decoder_input = layers.Input(K.int_shape(z)[1:])\n",
        "\n",
        "  # Expand to 784 total pixels\n",
        "  x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activation='relu')(decoder_input)\n",
        "\n",
        "  # reshape\n",
        "  x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "\n",
        "  # use Conv2DTranspose to reverse the conv layers from the encoder\n",
        "  x = layers.Conv3DTranspose(8, 3,\n",
        "                           padding='same', \n",
        "                           activation='relu',\n",
        "                           strides=(2, 2,2))(x)\n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='sigmoid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "  x = layers.Conv3DTranspose(8, 3,\n",
        "                           padding='same', \n",
        "                           activation='relu',\n",
        "                           strides=(2, 2,2))(x)\n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='sigmoid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  x = layers.Conv3DTranspose(8, 3,\n",
        "                           padding='same', \n",
        "                           activation='relu',\n",
        "                           strides=(2, 2,2))(x)\n",
        "  x = layers.Conv3D(8, 3,\n",
        "                  padding='same', \n",
        "                  activation='sigmoid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  # x = layers.Conv3DTranspose(8, 3,\n",
        "  #                          padding='same', \n",
        "  #                          activation='relu',\n",
        "  #                          strides=(2, 2,2))(x)\n",
        "  # x = layers.Conv3D(8, 3,\n",
        "  #                 padding='same', \n",
        "  #                 activation='sigmoid')(x)\n",
        "  # x = BatchNormalization()(x)\n",
        "  # x = layers.Conv3DTranspose(8, 3,\n",
        "  #                          padding='same', \n",
        "  #                          activation='relu',\n",
        "  #                          strides=(2, 2,2))(x)\n",
        "  x = layers.Conv3D(1, 3,\n",
        "                  padding='same', \n",
        "                  activation='sigmoid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = tf.keras.layers.ZeroPadding3D(padding=4)(x)\n",
        "  x = tf.keras.layers.Lambda(lambda x: x[:,4:44,4:260,4:260,:])(x)\n",
        "\n",
        "\n",
        "  # decoder model statement\n",
        "  decoder = Model(decoder_input, x)\n",
        "  # apply the decoder to the sample from the latent distribution\n",
        "  z_decoded = decoder(z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # construct a custom layer to calculate the loss\n",
        "  class CustomVariationalLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = K.flatten(x)\n",
        "        z_decoded = K.flatten(z_decoded)\n",
        "        # Reconstruction loss\n",
        "        xent_loss = keras.metrics.mean_absolute_error(x, z_decoded)\n",
        "        # KL divergence\n",
        "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    # adds the custom loss to the class\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x\n",
        "\n",
        "  # apply the custom loss to the input images and the decoded latent distribution sample\n",
        "  y = CustomVariationalLayer()([input_img, z_decoded])\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  model = Model(input_img ,y)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybWU8Vxa-bW7",
        "outputId": "d9e3582b-3ca8-4ab2-a0fd-c6887a3d304c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental_run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-cef63d6acc25>:2: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOrqb8w-Y1k5",
        "outputId": "ba224386-2c9f-4aec-9c33-8bb2b31be86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "mod=build_model()\n",
        "mod.compile(optimizer='adam',loss=None)\n",
        "mod.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 40, 256, 256 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 40, 256, 256, 224         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D)    (None, 20, 128, 128, 0           conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 20, 128, 128, 32          max_pooling3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 20, 128, 128, 1736        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3D)  (None, 10, 64, 64, 8 0           conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 10, 64, 64, 8 32          max_pooling3d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 10, 64, 64, 8 1736        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3D)  (None, 5, 32, 32, 8) 0           conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 5, 32, 32, 8) 32          max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 40960)        0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           1310752     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            66          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            66          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 2)            0           dense_1[0][0]                    \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "functional_1 (Functional)       (None, 40, 256, 256, 133613      lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer (Custo (None, 40, 256, 256, 0           input_1[0][0]                    \n",
            "                                                                 functional_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,448,289\n",
            "Trainable params: 1,448,191\n",
            "Non-trainable params: 98\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "outputId": "8722ce26-dac5-4235-a9cf-b85100dbc2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import gc\n",
        "mod.fit(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   6/2000 [..............................] - ETA: 14:06 - loss: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnHUDiVpmX2C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}