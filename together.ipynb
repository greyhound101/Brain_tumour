{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "together",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/together.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "outputId": "ad858d46-dc53-4a0d-fb9b-873b26dd0c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/gdrive/My Drive/lungs/train.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/gdrive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "outputId": "ea86106b-bffe-42e6-f36d-51c2f1c08437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 89kB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta, datetime\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pydicom\n",
        "import pytest\n",
        "import scipy.ndimage as ndimage\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from skimage import measure, morphology, segmentation\n",
        "from time import time, sleep\n",
        "from tqdm import trange, tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DistributedSampler, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "outputId": "07eced3e-d30a-44dc-8590-2ad8a0bfe43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_tab(df):\n",
        "    vector = [(df.Age.values[0] - 30) / 30] \n",
        "    \n",
        "    if df.Sex.values[0] == 'male':\n",
        "       vector.append(0)\n",
        "    else:\n",
        "       vector.append(1)\n",
        "    \n",
        "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
        "        vector.extend([0,0])\n",
        "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
        "        vector.extend([1,1])\n",
        "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
        "        vector.extend([0,1])\n",
        "    else:\n",
        "        vector.extend([1,0])\n",
        "    return np.array(vector) \n",
        "\n",
        "A = {} \n",
        "TAB = {} \n",
        "P = [] \n",
        "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
        "    sub = train.loc[train.Patient == p, :] \n",
        "    fvc = sub.FVC.values\n",
        "    weeks = sub.Weeks.values\n",
        "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
        "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
        "    \n",
        "    A[p] = a\n",
        "    TAB[p] = get_tab(sub)\n",
        "    P.append(p)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "176it [00:00, 730.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u"
      },
      "source": [
        "class CTTensorsDataset(Dataset):\n",
        "    def __init__(self,TAB,A, transform=None):\n",
        "        self.tensor_files = [Path(i) for i in glob.glob('/content/ID*')]\n",
        "        self.transform = transform\n",
        "        self.TAB=TAB\n",
        "        self.A=A\n",
        "    def __len__(self):\n",
        "        return len(self.tensor_files)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.tolist()\n",
        "\n",
        "        image = torch.load(self.tensor_files[item])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'patient_id': self.tensor_files[item].stem,\n",
        "            'image': image,\n",
        "            'tab':self.TAB[self.tensor_files[item].stem],\n",
        "            'slope':self.A[self.tensor_files[item].stem]\n",
        "        }\n",
        "\n",
        "    def mean(self):\n",
        "        cum = 0\n",
        "        for i in range(len(self)):\n",
        "            sample = self[i]['image']\n",
        "            cum += torch.mean(sample).item()\n",
        "\n",
        "        return cum / len(self)\n",
        "\n",
        "    def random_split(self, val_size: float):\n",
        "        num_val = int(val_size * len(self))\n",
        "        num_train = len(self) - num_val\n",
        "        return random_split(self, [num_train, num_val])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGULd4YJatQz"
      },
      "source": [
        "\n",
        "best_accuracy_last={}\n",
        "final_accuracy_last={}\n",
        "history_last={}\n",
        "answers_last={}\n",
        "predictions_last={}\n",
        "predictions_last_best={}\n",
        "times_last={}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L"
      },
      "source": [
        "class ZeroCenter:\n",
        "    def __init__(self, pre_calculated_mean):\n",
        "        self.pre_calculated_mean = pre_calculated_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor - self.pre_calculated_mean"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb"
      },
      "source": [
        "root_dir = '/kaggle/input/osic-cached-dataset'\n",
        "test_dir = '/kaggle/input/osic-pulmonary-fibrosis-progression/test'\n",
        "model_file = '/kaggle/working/diophantus.pt'\n",
        "resize_dims = (40, 256, 256)\n",
        "clip_bounds = (-1000, 200)\n",
        "watershed_iterations = 1\n",
        "pre_calculated_mean = 0.02865046213070556\n",
        "latent_features = 10\n",
        "batch_size = 16\n",
        "learning_rate = 3e-3\n",
        "num_epochs = 10\n",
        "val_size = 0.2\n",
        "tensorboard_dir = '/kaggle/working/runs'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru"
      },
      "source": [
        "import glob\n",
        "train = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "# cum = 0\n",
        "# for i in range(len(train)):\n",
        "#     sample = train[i]['image']\n",
        "#     cum += torch.mean(sample).item()\n",
        "\n",
        "# assert cum / len(train) == pytest.approx(0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB"
      },
      "source": [
        "class VarAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_features=latent_features):\n",
        "        super(VarAutoEncoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv3d(1, 16, 3)\n",
        "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(32, 96, 2)\n",
        "        self.conv4 = nn.Conv3d(96, 1, 1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=3, stride=3, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.fc1 = nn.Linear(10 * 10, latent_features)\n",
        "        self.fc2 = nn.Linear(10 * 10, latent_features)\n",
        "\n",
        "        #tabular\n",
        "        self.tab1=nn.Linear(104,256)\n",
        "        self.tab2=nn.Linear(256,256)\n",
        "        self.tab3=nn.Linear(256,100)\n",
        "\n",
        "\n",
        "        #output\n",
        "        self.out1=nn.Linear(100,256)\n",
        "        self.out2=nn.Linear(256,256)\n",
        "        self.out3=nn.Linear(256,1)\n",
        "\n",
        "\n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_features, 10 * 10)\n",
        "        self.deconv0 = nn.ConvTranspose3d(1, 96, 1)\n",
        "        self.deconv1 = nn.ConvTranspose3d(96, 32, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 16, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(16, 1, 3)\n",
        "        self.unpool0 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=3, stride=3)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def encode(self, x,y, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.act(self.conv1(x))\n",
        "        up3out_shape = x.shape\n",
        "        x, i1 = self.pool1(x)\n",
        "\n",
        "        x = self.act(self.conv2(x))\n",
        "        up2out_shape = x.shape\n",
        "        x, i2 = self.pool2(x)\n",
        "\n",
        "        x = self.act(self.conv3(x))\n",
        "        up1out_shape = x.shape\n",
        "        x, i3 = self.pool3(x)\n",
        "\n",
        "        x = self.act(self.conv4(x))\n",
        "        up0out_shape = x.shape\n",
        "        x, i4 = self.pool4(x)\n",
        "\n",
        "        x = x.view(-1, 10 * 10)\n",
        "        x = torch.cat((x,y),1)\n",
        "        x = self.act(self.tab1(x))\n",
        "        x = self.act(self.tab2(x))\n",
        "        x = self.act(self.tab3(x))\n",
        "\n",
        "        mu = self.act(self.fc1(x))\n",
        "        log_var = self.act(self.fc2(x))\n",
        "        \n",
        "        if return_partials:\n",
        "            \n",
        "            return mu, log_var, up3out_shape, i1, up2out_shape, i2, up1out_shape, i3, \\\n",
        "                   up0out_shape, i4,x\n",
        "\n",
        "        else:\n",
        "            return mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x,y):\n",
        "        mu, log_var, up3out_shape, i1, up2out_shape, i2, \\\n",
        "        up1out_shape, i3, up0out_shape, i4,out = self.encode(x,y)\n",
        "        \n",
        "        out1=self.act(self.out1(out))\n",
        "        out2=self.act(self.out2(out1))\n",
        "        output=self.out3(out2)\n",
        "\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "       \n",
        "        # Decoder\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = x.view(-1, 1, 1, 10, 10)\n",
        "        x = self.unpool0(x, output_size=up0out_shape, indices=i4)\n",
        "        x = self.act(self.deconv0(x))\n",
        "        x = self.unpool1(x, output_size=up1out_shape, indices=i3)\n",
        "        x = self.act(self.deconv1(x))\n",
        "        x = self.unpool2(x, output_size=up2out_shape, indices=i2)\n",
        "        x = self.act(self.deconv2(x))\n",
        "        x = self.unpool3(x, output_size=up3out_shape, indices=i1)\n",
        "        x = self.act((self.deconv3(x)))\n",
        "\n",
        "        return x, mu, log_var,output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        "t0 = time()\n",
        "\n",
        "# Load the data\n",
        "data = CTTensorsDataset(TAB,A,\n",
        "    transform=ZeroCenter(pre_calculated_mean=pre_calculated_mean)\n",
        ")\n",
        "train_set, val_set = data.random_split(val_size)\n",
        "datasets = {'train': train_set, 'val': val_set}\n",
        "dataloaders = {\n",
        "    x: DataLoader(\n",
        "        datasets[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(x == 'train'),\n",
        "        num_workers=2\n",
        "    ) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Prepare for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VarAutoEncoder(latent_features=latent_features).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
        "best_model_wts = None\n",
        "best_loss = np.inf\n",
        "\n",
        "date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "log_dir = Path(tensorboard_dir) / f'{date_time}'\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O2AHP_NZtDk",
        "outputId": "4ff5de99-6a3a-4eac-d2db-500c71ea4236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss=  {'train':[],'val':[]}\n",
        "for epoch in range(300):\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_preds = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        bar = tqdm(dataloaders[phase])\n",
        "        for inputs in bar:\n",
        "            bar.set_description(f'Epoch {epoch + 1} {phase}'.ljust(20))\n",
        "            tabular=inputs['tab'].to(device, dtype=torch.float)\n",
        "            slopes_input=inputs['slope'].to(device, dtype=torch.float)\n",
        "            inputs = inputs['image'].to(device, dtype=torch.float)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs, mu, log_var,slope_output = model(inputs,tabular)\n",
        "                \n",
        "                # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "                reconst_loss = F.mse_loss(outputs, inputs, size_average=False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                slope_loss = F.mse_loss(slope_output,slopes_input)\n",
        "                \n",
        "                loss =  reconst_loss + kl_div+slope_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_preds += inputs.size(0)\n",
        "            bar.set_postfix(loss=f'{running_loss / running_preds:0.6f}')\n",
        "        total_loss[phase].append(loss.item()  )\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "        scheduler.step()\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, model_file)\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Done! Time {timedelta(seconds=time() - t0)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 train       :   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       :  89%|████████▉ | 8/9 [00:31<00:03,  3.76s/it, loss=579972.359375]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 train       : 100%|██████████| 9/9 [00:35<00:00,  3.90s/it, loss=573753.016179]\n",
            "Epoch 1 val         :  67%|██████▋   | 2/3 [00:01<00:01,  1.02s/it, loss=549239.906250]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Epoch 1 val         : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=511164.524330]\n",
            "Epoch 2 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=521378.568262]\n",
            "Epoch 2 val         : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=449057.837054]\n",
            "Epoch 3 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=456910.230053]\n",
            "Epoch 3 val         : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=394345.095536]\n",
            "Epoch 4 train       : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=407191.010195]\n",
            "Epoch 4 val         : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=367425.739955]\n",
            "Epoch 5 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=383921.759973]\n",
            "Epoch 5 val         : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=337188.271205]\n",
            "Epoch 6 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=357923.488032]\n",
            "Epoch 6 val         : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=323797.794754]\n",
            "Epoch 7 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=312442.537012]\n",
            "Epoch 7 val         : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=275502.986942]\n",
            "Epoch 8 train       : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=311318.161680]\n",
            "Epoch 8 val         : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=274021.126228]\n",
            "Epoch 9 train       : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=282997.966201]\n",
            "Epoch 9 val         : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=278413.679464]\n",
            "Epoch 10 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=275282.665004]\n",
            "Epoch 10 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=249686.117188]\n",
            "Epoch 11 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=257233.550089]\n",
            "Epoch 11 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=229007.194978]\n",
            "Epoch 12 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=236752.032247]\n",
            "Epoch 12 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=211538.882366]\n",
            "Epoch 13 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=221997.688608]\n",
            "Epoch 13 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=201231.400893]\n",
            "Epoch 14 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=210943.532247]\n",
            "Epoch 14 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=191343.773549]\n",
            "Epoch 15 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=201347.997340]\n",
            "Epoch 15 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=182970.412612]\n",
            "Epoch 16 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=193024.007092]\n",
            "Epoch 16 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=175923.146652]\n",
            "Epoch 17 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=186096.269504]\n",
            "Epoch 17 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=170379.926451]\n",
            "Epoch 18 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=180558.957668]\n",
            "Epoch 18 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=165567.225725]\n",
            "Epoch 19 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=175497.212544]\n",
            "Epoch 19 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=160393.071205]\n",
            "Epoch 20 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=170975.906804]\n",
            "Epoch 20 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=156796.757478]\n",
            "Epoch 21 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=166981.881316]\n",
            "Epoch 21 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=153425.191741]\n",
            "Epoch 22 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=163165.867631]\n",
            "Epoch 22 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=149881.452121]\n",
            "Epoch 23 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=159909.373781]\n",
            "Epoch 23 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=147298.856696]\n",
            "Epoch 24 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=157347.966645]\n",
            "Epoch 24 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=144900.200279]\n",
            "Epoch 25 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=154262.183289]\n",
            "Epoch 25 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=142620.801451]\n",
            "Epoch 26 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=152403.304355]\n",
            "Epoch 26 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=140757.273382]\n",
            "Epoch 27 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=150298.449911]\n",
            "Epoch 27 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=139790.357757]\n",
            "Epoch 28 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=149515.511802]\n",
            "Epoch 28 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=138110.640513]\n",
            "Epoch 29 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=147843.138575]\n",
            "Epoch 29 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=136240.459375]\n",
            "Epoch 30 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=146645.601064]\n",
            "Epoch 30 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=135220.129576]\n",
            "Epoch 31 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=145328.141678]\n",
            "Epoch 31 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=134177.597712]\n",
            "Epoch 32 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=143737.690603]\n",
            "Epoch 32 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=132592.800446]\n",
            "Epoch 33 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=142215.999778]\n",
            "Epoch 33 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=131680.568415]\n",
            "Epoch 34 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=141146.456339]\n",
            "Epoch 34 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=130149.367746]\n",
            "Epoch 35 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=139985.820867]\n",
            "Epoch 35 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=128930.221987]\n",
            "Epoch 36 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=138796.502660]\n",
            "Epoch 36 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=128649.849554]\n",
            "Epoch 37 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=138487.720357]\n",
            "Epoch 37 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=127372.493192]\n",
            "Epoch 38 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=137418.799258]\n",
            "Epoch 38 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=126346.715011]\n",
            "Epoch 39 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=136200.625332]\n",
            "Epoch 39 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=126023.373270]\n",
            "Epoch 40 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=135772.461492]\n",
            "Epoch 40 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=125026.010714]\n",
            "Epoch 41 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=134630.553967]\n",
            "Epoch 41 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=124396.907980]\n",
            "Epoch 42 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=133963.185395]\n",
            "Epoch 42 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=123830.395982]\n",
            "Epoch 43 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=133482.443096]\n",
            "Epoch 43 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=123288.564955]\n",
            "Epoch 44 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=133057.544437]\n",
            "Epoch 44 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=122748.418917]\n",
            "Epoch 45 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=132405.840980]\n",
            "Epoch 45 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=122283.331473]\n",
            "Epoch 46 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=132072.551529]\n",
            "Epoch 46 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=121771.081473]\n",
            "Epoch 47 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=131440.336215]\n",
            "Epoch 47 val        : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=121310.991741]\n",
            "Epoch 48 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=130934.309951]\n",
            "Epoch 48 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=120986.365179]\n",
            "Epoch 49 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=130688.028424]\n",
            "Epoch 49 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=120590.049944]\n",
            "Epoch 50 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129904.042276]\n",
            "Epoch 50 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=120298.747266]\n",
            "Epoch 51 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=129708.021997]\n",
            "Epoch 51 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=120055.664453]\n",
            "Epoch 52 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129347.033577]\n",
            "Epoch 52 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=119725.278237]\n",
            "Epoch 53 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129210.799867]\n",
            "Epoch 53 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=119535.466574]\n",
            "Epoch 54 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=128966.909685]\n",
            "Epoch 54 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=119243.891574]\n",
            "Epoch 55 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=129088.308898]\n",
            "Epoch 55 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=119053.253013]\n",
            "Epoch 56 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=128639.045767]\n",
            "Epoch 56 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=118794.721596]\n",
            "Epoch 57 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=128040.147828]\n",
            "Epoch 57 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=118556.239732]\n",
            "Epoch 58 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=127827.947972]\n",
            "Epoch 58 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=118270.359821]\n",
            "Epoch 59 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=127502.135084]\n",
            "Epoch 59 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=118047.239621]\n",
            "Epoch 60 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=127525.710328]\n",
            "Epoch 60 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=117798.875949]\n",
            "Epoch 61 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=126965.406084]\n",
            "Epoch 61 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=117608.715904]\n",
            "Epoch 62 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=126858.361868]\n",
            "Epoch 62 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=117510.841127]\n",
            "Epoch 63 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=126604.045490]\n",
            "Epoch 63 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=117304.113672]\n",
            "Epoch 64 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=126300.107325]\n",
            "Epoch 64 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=117078.543248]\n",
            "Epoch 65 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=126002.971133]\n",
            "Epoch 65 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=117044.806083]\n",
            "Epoch 66 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=125662.185228]\n",
            "Epoch 66 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=116783.734096]\n",
            "Epoch 67 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=125929.583223]\n",
            "Epoch 67 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=116593.455301]\n",
            "Epoch 68 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=125854.189550]\n",
            "Epoch 68 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=116461.118025]\n",
            "Epoch 69 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=125490.946864]\n",
            "Epoch 69 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=116288.467801]\n",
            "Epoch 70 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=125026.625997]\n",
            "Epoch 70 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=116186.064342]\n",
            "Epoch 71 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=125305.596465]\n",
            "Epoch 71 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=116031.206083]\n",
            "Epoch 72 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=125337.105663]\n",
            "Epoch 72 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=115913.490179]\n",
            "Epoch 73 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124906.930408]\n",
            "Epoch 73 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=115854.827511]\n",
            "Epoch 74 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=125041.222518]\n",
            "Epoch 74 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=115670.343917]\n",
            "Epoch 75 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=124644.166390]\n",
            "Epoch 75 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=115626.281417]\n",
            "Epoch 76 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124443.249113]\n",
            "Epoch 76 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=115485.106752]\n",
            "Epoch 77 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=124710.909242]\n",
            "Epoch 77 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=115384.100725]\n",
            "Epoch 78 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124529.623726]\n",
            "Epoch 78 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=115262.262221]\n",
            "Epoch 79 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124399.546432]\n",
            "Epoch 79 val        : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=115166.052846]\n",
            "Epoch 80 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=124270.191046]\n",
            "Epoch 80 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=115146.880859]\n",
            "Epoch 81 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123958.118573]\n",
            "Epoch 81 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=115039.924665]\n",
            "Epoch 82 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123805.375887]\n",
            "Epoch 82 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114986.385603]\n",
            "Epoch 83 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=124182.350177]\n",
            "Epoch 83 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=114931.299833]\n",
            "Epoch 84 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123859.966146]\n",
            "Epoch 84 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114888.887333]\n",
            "Epoch 85 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123474.018285]\n",
            "Epoch 85 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=114782.177176]\n",
            "Epoch 86 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123790.721465]\n",
            "Epoch 86 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114693.723047]\n",
            "Epoch 87 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123595.199856]\n",
            "Epoch 87 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114696.590904]\n",
            "Epoch 88 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123576.992686]\n",
            "Epoch 88 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114633.289342]\n",
            "Epoch 89 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123767.768949]\n",
            "Epoch 89 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114586.200725]\n",
            "Epoch 90 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123480.057846]\n",
            "Epoch 90 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114489.807478]\n",
            "Epoch 91 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123471.631372]\n",
            "Epoch 91 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114499.784319]\n",
            "Epoch 92 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123687.189439]\n",
            "Epoch 92 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114410.964286]\n",
            "Epoch 93 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123146.908522]\n",
            "Epoch 93 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=114379.694754]\n",
            "Epoch 94 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123125.992520]\n",
            "Epoch 94 val        : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=114345.372321]\n",
            "Epoch 95 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=123035.161569]\n",
            "Epoch 95 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114306.231920]\n",
            "Epoch 96 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122855.160406]\n",
            "Epoch 96 val        : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114230.248717]\n",
            "Epoch 97 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123003.494459]\n",
            "Epoch 97 val        : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114178.103516]\n",
            "Epoch 98 train      : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122927.464871]\n",
            "Epoch 98 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=114177.523158]\n",
            "Epoch 99 train      : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123062.187943]\n",
            "Epoch 99 val        : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=114092.759263]\n",
            "Epoch 100 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=123047.009142]\n",
            "Epoch 100 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=114054.032701]\n",
            "Epoch 101 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122848.697086]\n",
            "Epoch 101 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=114020.466853]\n",
            "Epoch 102 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122713.313387]\n",
            "Epoch 102 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113996.671484]\n",
            "Epoch 103 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122661.220412]\n",
            "Epoch 103 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113976.774833]\n",
            "Epoch 104 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122678.358488]\n",
            "Epoch 104 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113861.048549]\n",
            "Epoch 105 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122922.806904]\n",
            "Epoch 105 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113885.392913]\n",
            "Epoch 106 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122744.167664]\n",
            "Epoch 106 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113835.652009]\n",
            "Epoch 107 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122512.819648]\n",
            "Epoch 107 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113801.846596]\n",
            "Epoch 108 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122656.368240]\n",
            "Epoch 108 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113825.012667]\n",
            "Epoch 109 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122615.584940]\n",
            "Epoch 109 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113729.713058]\n",
            "Epoch 110 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122562.564661]\n",
            "Epoch 110 val       : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=113754.993192]\n",
            "Epoch 111 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122618.642232]\n",
            "Epoch 111 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113726.173326]\n",
            "Epoch 112 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122693.201130]\n",
            "Epoch 112 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113698.546150]\n",
            "Epoch 113 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122644.049978]\n",
            "Epoch 113 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=113686.439342]\n",
            "Epoch 114 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122586.673482]\n",
            "Epoch 114 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=113646.901618]\n",
            "Epoch 115 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122388.079344]\n",
            "Epoch 115 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113608.244922]\n",
            "Epoch 116 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122365.785516]\n",
            "Epoch 116 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113627.613616]\n",
            "Epoch 117 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122337.610206]\n",
            "Epoch 117 val       : 100%|██████████| 3/3 [00:01<00:00,  1.55it/s, loss=113598.989230]\n",
            "Epoch 118 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122496.920047]\n",
            "Epoch 118 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=113566.228069]\n",
            "Epoch 119 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122304.609375]\n",
            "Epoch 119 val       : 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=113590.746596]\n",
            "Epoch 120 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122357.126441]\n",
            "Epoch 120 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=113538.208259]\n",
            "Epoch 121 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122343.493185]\n",
            "Epoch 121 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113498.395536]\n",
            "Epoch 122 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122525.834996]\n",
            "Epoch 122 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113515.997824]\n",
            "Epoch 123 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122197.853557]\n",
            "Epoch 123 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=113483.805022]\n",
            "Epoch 124 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122271.017897]\n",
            "Epoch 124 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113488.402511]\n",
            "Epoch 125 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122330.072861]\n",
            "Epoch 125 val       : 100%|██████████| 3/3 [00:01<00:00,  1.57it/s, loss=113440.269922]\n",
            "Epoch 126 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122240.687722]\n",
            "Epoch 126 val       : 100%|██████████| 3/3 [00:01<00:00,  1.54it/s, loss=113449.780022]\n",
            "Epoch 127 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122218.981549]\n",
            "Epoch 127 val       : 100%|██████████| 3/3 [00:01<00:00,  1.58it/s, loss=113418.078516]\n",
            "Epoch 128 train     : 100%|██████████| 9/9 [00:30<00:00,  3.43s/it, loss=122358.206062]\n",
            "Epoch 128 val       : 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, loss=113413.211719]\n",
            "Epoch 129 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122390.746565]\n",
            "Epoch 129 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113407.414676]\n",
            "Epoch 130 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122124.097462]\n",
            "Epoch 130 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113362.663728]\n",
            "Epoch 131 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121957.244404]\n",
            "Epoch 131 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113365.666685]\n",
            "Epoch 132 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122387.814550]\n",
            "Epoch 132 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113383.486272]\n",
            "Epoch 133 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122432.748726]\n",
            "Epoch 133 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113349.756138]\n",
            "Epoch 134 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122160.712821]\n",
            "Epoch 134 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113342.651953]\n",
            "Epoch 135 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121934.419492]\n",
            "Epoch 135 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113326.806864]\n",
            "Epoch 136 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=122323.456727]\n",
            "Epoch 136 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113330.175279]\n",
            "Epoch 137 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122393.302250]\n",
            "Epoch 137 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113339.222600]\n",
            "Epoch 138 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122133.843251]\n",
            "Epoch 138 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113336.032812]\n",
            "Epoch 139 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122278.161569]\n",
            "Epoch 139 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113274.255413]\n",
            "Epoch 140 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122058.898604]\n",
            "Epoch 140 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113267.440290]\n",
            "Epoch 141 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=122045.799368]\n",
            "Epoch 141 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113301.991853]\n",
            "Epoch 142 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121772.257037]\n",
            "Epoch 142 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113258.356808]\n",
            "Epoch 143 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122157.779311]\n",
            "Epoch 143 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113299.269475]\n",
            "Epoch 144 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122068.182458]\n",
            "Epoch 144 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113315.834319]\n",
            "Epoch 145 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121764.142287]\n",
            "Epoch 145 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113239.630190]\n",
            "Epoch 146 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122124.704178]\n",
            "Epoch 146 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113246.370313]\n",
            "Epoch 147 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122025.264129]\n",
            "Epoch 147 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113268.324609]\n",
            "Epoch 148 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121938.538065]\n",
            "Epoch 148 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113250.595647]\n",
            "Epoch 149 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121837.722795]\n",
            "Epoch 149 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113235.414621]\n",
            "Epoch 150 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122048.538841]\n",
            "Epoch 150 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113262.876897]\n",
            "Epoch 151 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121967.161846]\n",
            "Epoch 151 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113210.393415]\n",
            "Epoch 152 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121903.089040]\n",
            "Epoch 152 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113212.867299]\n",
            "Epoch 153 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122313.243794]\n",
            "Epoch 153 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=113205.858873]\n",
            "Epoch 154 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122141.094082]\n",
            "Epoch 154 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113241.025837]\n",
            "Epoch 155 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122370.321199]\n",
            "Epoch 155 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113253.317187]\n",
            "Epoch 156 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122052.852892]\n",
            "Epoch 156 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113215.848884]\n",
            "Epoch 157 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122273.712988]\n",
            "Epoch 157 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113204.657645]\n",
            "Epoch 158 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122030.688941]\n",
            "Epoch 158 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113235.430859]\n",
            "Epoch 159 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121755.285018]\n",
            "Epoch 159 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113191.413393]\n",
            "Epoch 160 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121733.397385]\n",
            "Epoch 160 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113229.555078]\n",
            "Epoch 161 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122106.372895]\n",
            "Epoch 161 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113214.955915]\n",
            "Epoch 162 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122021.266179]\n",
            "Epoch 162 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113227.110435]\n",
            "Epoch 163 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122090.294049]\n",
            "Epoch 163 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113241.222768]\n",
            "Epoch 164 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121664.625776]\n",
            "Epoch 164 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113184.523493]\n",
            "Epoch 165 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121900.682569]\n",
            "Epoch 165 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113236.136161]\n",
            "Epoch 166 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121871.100399]\n",
            "Epoch 166 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113182.271819]\n",
            "Epoch 167 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121904.571476]\n",
            "Epoch 167 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113182.264230]\n",
            "Epoch 168 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121627.240969]\n",
            "Epoch 168 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113173.001842]\n",
            "Epoch 169 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121833.109320]\n",
            "Epoch 169 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113163.903348]\n",
            "Epoch 170 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121951.126551]\n",
            "Epoch 170 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113162.981194]\n",
            "Epoch 171 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121693.575576]\n",
            "Epoch 171 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113162.585491]\n",
            "Epoch 172 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121810.948138]\n",
            "Epoch 172 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113161.226507]\n",
            "Epoch 173 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122102.948471]\n",
            "Epoch 173 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113223.403795]\n",
            "Epoch 174 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121985.064218]\n",
            "Epoch 174 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113133.576563]\n",
            "Epoch 175 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122151.023050]\n",
            "Epoch 175 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113142.672321]\n",
            "Epoch 176 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121856.758200]\n",
            "Epoch 176 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=113115.616853]\n",
            "Epoch 177 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122052.040669]\n",
            "Epoch 177 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113179.640960]\n",
            "Epoch 178 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121889.916445]\n",
            "Epoch 178 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113114.132199]\n",
            "Epoch 179 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121861.910073]\n",
            "Epoch 179 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113140.690848]\n",
            "Epoch 180 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121905.384364]\n",
            "Epoch 180 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113144.500000]\n",
            "Epoch 181 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121846.245235]\n",
            "Epoch 181 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113156.090290]\n",
            "Epoch 182 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121908.092143]\n",
            "Epoch 182 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113104.858984]\n",
            "Epoch 183 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122008.549147]\n",
            "Epoch 183 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113121.242634]\n",
            "Epoch 184 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122054.609430]\n",
            "Epoch 184 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113116.670089]\n",
            "Epoch 185 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121831.150044]\n",
            "Epoch 185 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113127.010379]\n",
            "Epoch 186 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121941.304854]\n",
            "Epoch 186 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113158.299219]\n",
            "Epoch 187 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121918.754876]\n",
            "Epoch 187 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113107.863337]\n",
            "Epoch 188 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122173.954787]\n",
            "Epoch 188 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113142.994810]\n",
            "Epoch 189 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121980.547595]\n",
            "Epoch 189 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113137.030525]\n",
            "Epoch 190 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121722.375443]\n",
            "Epoch 190 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113123.122545]\n",
            "Epoch 191 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121955.153369]\n",
            "Epoch 191 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113127.359542]\n",
            "Epoch 192 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121941.818983]\n",
            "Epoch 192 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113093.184766]\n",
            "Epoch 193 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122073.553856]\n",
            "Epoch 193 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113114.815067]\n",
            "Epoch 194 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122153.135749]\n",
            "Epoch 194 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113100.483761]\n",
            "Epoch 195 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122035.778424]\n",
            "Epoch 195 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113110.667690]\n",
            "Epoch 196 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121891.430408]\n",
            "Epoch 196 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113112.276730]\n",
            "Epoch 197 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121868.913896]\n",
            "Epoch 197 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113177.598270]\n",
            "Epoch 198 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122330.749446]\n",
            "Epoch 198 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113112.960603]\n",
            "Epoch 199 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121832.734486]\n",
            "Epoch 199 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113128.482701]\n",
            "Epoch 200 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121796.141567]\n",
            "Epoch 200 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113095.696094]\n",
            "Epoch 201 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121934.500166]\n",
            "Epoch 201 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113122.132924]\n",
            "Epoch 202 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121950.586159]\n",
            "Epoch 202 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113137.868583]\n",
            "Epoch 203 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121751.845357]\n",
            "Epoch 203 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113104.942857]\n",
            "Epoch 204 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121618.209940]\n",
            "Epoch 204 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113148.131362]\n",
            "Epoch 205 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121718.439605]\n",
            "Epoch 205 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113122.721373]\n",
            "Epoch 206 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121663.510029]\n",
            "Epoch 206 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113081.916908]\n",
            "Epoch 207 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121580.490581]\n",
            "Epoch 207 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113112.326507]\n",
            "Epoch 208 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121901.104610]\n",
            "Epoch 208 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113127.570089]\n",
            "Epoch 209 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121749.512578]\n",
            "Epoch 209 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113113.476562]\n",
            "Epoch 210 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122144.399213]\n",
            "Epoch 210 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113111.456808]\n",
            "Epoch 211 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121911.451574]\n",
            "Epoch 211 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113143.776228]\n",
            "Epoch 212 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121612.107657]\n",
            "Epoch 212 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113096.079018]\n",
            "Epoch 213 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121832.542553]\n",
            "Epoch 213 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113098.675279]\n",
            "Epoch 214 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121908.179078]\n",
            "Epoch 214 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113096.022321]\n",
            "Epoch 215 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122161.306184]\n",
            "Epoch 215 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113074.239453]\n",
            "Epoch 216 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121940.499113]\n",
            "Epoch 216 val       : 100%|██████████| 3/3 [00:02<00:00,  1.45it/s, loss=113111.631473]\n",
            "Epoch 217 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122075.377826]\n",
            "Epoch 217 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113145.179185]\n",
            "Epoch 218 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=121750.044326]\n",
            "Epoch 218 val       : 100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=113096.129297]\n",
            "Epoch 219 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121956.667442]\n",
            "Epoch 219 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113143.984263]\n",
            "Epoch 220 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121853.256372]\n",
            "Epoch 220 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113181.392132]\n",
            "Epoch 221 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121642.397551]\n",
            "Epoch 221 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113084.996652]\n",
            "Epoch 222 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122124.661292]\n",
            "Epoch 222 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113133.015681]\n",
            "Epoch 223 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122067.108544]\n",
            "Epoch 223 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113085.365179]\n",
            "Epoch 224 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121771.167719]\n",
            "Epoch 224 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113153.567913]\n",
            "Epoch 225 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121939.196421]\n",
            "Epoch 225 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113150.396540]\n",
            "Epoch 226 train     : 100%|██████████| 9/9 [00:31<00:00,  3.46s/it, loss=121959.345911]\n",
            "Epoch 226 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113071.815011]\n",
            "Epoch 227 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121863.326574]\n",
            "Epoch 227 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113099.070089]\n",
            "Epoch 228 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121655.808012]\n",
            "Epoch 228 val       : 100%|██████████| 3/3 [00:02<00:00,  1.47it/s, loss=113135.762444]\n",
            "Epoch 229 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122040.590426]\n",
            "Epoch 229 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113138.922321]\n",
            "Epoch 230 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121800.609486]\n",
            "Epoch 230 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113151.682533]\n",
            "Epoch 231 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122093.546598]\n",
            "Epoch 231 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113136.196708]\n",
            "Epoch 232 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121883.596742]\n",
            "Epoch 232 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113132.335212]\n",
            "Epoch 233 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121956.783743]\n",
            "Epoch 233 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113120.300949]\n",
            "Epoch 234 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121649.316933]\n",
            "Epoch 234 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113134.349386]\n",
            "Epoch 235 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121681.728945]\n",
            "Epoch 235 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113100.604074]\n",
            "Epoch 236 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122006.011913]\n",
            "Epoch 236 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113120.271819]\n",
            "Epoch 237 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121866.253047]\n",
            "Epoch 237 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113081.585435]\n",
            "Epoch 238 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121905.868240]\n",
            "Epoch 238 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113111.328069]\n",
            "Epoch 239 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121689.955175]\n",
            "Epoch 239 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113097.169029]\n",
            "Epoch 240 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121776.181627]\n",
            "Epoch 240 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113084.655804]\n",
            "Epoch 241 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121950.592420]\n",
            "Epoch 241 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113098.205301]\n",
            "Epoch 242 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121797.873726]\n",
            "Epoch 242 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113092.976004]\n",
            "Epoch 243 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121668.808123]\n",
            "Epoch 243 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113135.787054]\n",
            "Epoch 244 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121865.212821]\n",
            "Epoch 244 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113142.456864]\n",
            "Epoch 245 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121961.427637]\n",
            "Epoch 245 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113076.629576]\n",
            "Epoch 246 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121797.510971]\n",
            "Epoch 246 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113145.766964]\n",
            "Epoch 247 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122034.134586]\n",
            "Epoch 247 val       : 100%|██████████| 3/3 [00:01<00:00,  1.52it/s, loss=113120.569531]\n",
            "Epoch 248 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121916.458500]\n",
            "Epoch 248 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113104.463951]\n",
            "Epoch 249 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122268.575576]\n",
            "Epoch 249 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113114.537444]\n",
            "Epoch 250 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121987.731272]\n",
            "Epoch 250 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113084.121652]\n",
            "Epoch 251 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121771.906028]\n",
            "Epoch 251 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113133.371819]\n",
            "Epoch 252 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121737.435450]\n",
            "Epoch 252 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113069.340290]\n",
            "Epoch 253 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122190.631483]\n",
            "Epoch 253 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113132.858147]\n",
            "Epoch 254 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121841.493573]\n",
            "Epoch 254 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113088.208650]\n",
            "Epoch 255 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121701.834331]\n",
            "Epoch 255 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113117.893750]\n",
            "Epoch 256 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121743.258976]\n",
            "Epoch 256 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113104.675781]\n",
            "Epoch 257 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122080.365581]\n",
            "Epoch 257 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113100.349051]\n",
            "Epoch 258 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121691.882868]\n",
            "Epoch 258 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113086.179911]\n",
            "Epoch 259 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121934.387799]\n",
            "Epoch 259 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113080.127846]\n",
            "Epoch 260 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122043.483211]\n",
            "Epoch 260 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113107.520592]\n",
            "Epoch 261 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121964.160516]\n",
            "Epoch 261 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113150.096652]\n",
            "Epoch 262 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122045.234430]\n",
            "Epoch 262 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113089.755022]\n",
            "Epoch 263 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121811.500665]\n",
            "Epoch 263 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113107.197600]\n",
            "Epoch 264 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121993.859874]\n",
            "Epoch 264 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113108.131417]\n",
            "Epoch 265 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121570.708887]\n",
            "Epoch 265 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113116.917746]\n",
            "Epoch 266 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122095.669825]\n",
            "Epoch 266 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113111.116741]\n",
            "Epoch 267 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122018.170047]\n",
            "Epoch 267 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113078.304408]\n",
            "Epoch 268 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121764.465592]\n",
            "Epoch 268 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113085.742355]\n",
            "Epoch 269 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121794.550144]\n",
            "Epoch 269 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113122.951339]\n",
            "Epoch 270 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122043.293052]\n",
            "Epoch 270 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113092.828125]\n",
            "Epoch 271 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121571.564495]\n",
            "Epoch 271 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113113.167913]\n",
            "Epoch 272 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122153.130375]\n",
            "Epoch 272 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113129.321763]\n",
            "Epoch 273 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121882.828180]\n",
            "Epoch 273 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113091.229799]\n",
            "Epoch 274 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121606.979388]\n",
            "Epoch 274 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113113.659487]\n",
            "Epoch 275 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121870.840315]\n",
            "Epoch 275 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113126.780804]\n",
            "Epoch 276 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121998.489860]\n",
            "Epoch 276 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113133.365960]\n",
            "Epoch 277 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=122038.376718]\n",
            "Epoch 277 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113068.693192]\n",
            "Epoch 278 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122264.437057]\n",
            "Epoch 278 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113112.476897]\n",
            "Epoch 279 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121609.137301]\n",
            "Epoch 279 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113084.911272]\n",
            "Epoch 280 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121934.288841]\n",
            "Epoch 280 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113093.484598]\n",
            "Epoch 281 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122077.423537]\n",
            "Epoch 281 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113080.660993]\n",
            "Epoch 282 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121761.947861]\n",
            "Epoch 282 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113066.652455]\n",
            "Epoch 283 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121852.516899]\n",
            "Epoch 283 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113117.501674]\n",
            "Epoch 284 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121735.132037]\n",
            "Epoch 284 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113124.608147]\n",
            "Epoch 285 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121950.569315]\n",
            "Epoch 285 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113126.547935]\n",
            "Epoch 286 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121916.150100]\n",
            "Epoch 286 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113124.261105]\n",
            "Epoch 287 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121576.422318]\n",
            "Epoch 287 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113093.174442]\n",
            "Epoch 288 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121792.331339]\n",
            "Epoch 288 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113113.827734]\n",
            "Epoch 289 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121910.907358]\n",
            "Epoch 289 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113091.987946]\n",
            "Epoch 290 train     : 100%|██████████| 9/9 [00:31<00:00,  3.44s/it, loss=121555.810395]\n",
            "Epoch 290 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113106.424107]\n",
            "Epoch 291 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122031.738697]\n",
            "Epoch 291 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113112.584654]\n",
            "Epoch 292 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121912.568484]\n",
            "Epoch 292 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113075.724219]\n",
            "Epoch 293 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121622.604388]\n",
            "Epoch 293 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113087.992690]\n",
            "Epoch 294 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=122116.669382]\n",
            "Epoch 294 val       : 100%|██████████| 3/3 [00:02<00:00,  1.49it/s, loss=113095.299107]\n",
            "Epoch 295 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121951.067210]\n",
            "Epoch 295 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113057.903739]\n",
            "Epoch 296 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121590.512910]\n",
            "Epoch 296 val       : 100%|██████████| 3/3 [00:01<00:00,  1.51it/s, loss=113101.963337]\n",
            "Epoch 297 train     : 100%|██████████| 9/9 [00:30<00:00,  3.44s/it, loss=121888.586104]\n",
            "Epoch 297 val       : 100%|██████████| 3/3 [00:02<00:00,  1.50it/s, loss=113090.257254]\n",
            "Epoch 298 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121918.124945]\n",
            "Epoch 298 val       : 100%|██████████| 3/3 [00:02<00:00,  1.48it/s, loss=113082.844308]\n",
            "Epoch 299 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=121949.590370]\n",
            "Epoch 299 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113150.185156]\n",
            "Epoch 300 train     : 100%|██████████| 9/9 [00:31<00:00,  3.45s/it, loss=122061.182680]\n",
            "Epoch 300 val       : 100%|██████████| 3/3 [00:01<00:00,  1.50it/s, loss=113118.257812]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done! Time 2:45:15.930966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyRfuRyGZtAl",
        "outputId": "74d9a586-fdb7-460d-f105-d3f5fed7c8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(y=total_loss['train'],x=list(range(len(total_loss['train']))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e24d85ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Dh1-V5Zs9i",
        "outputId": "40d7ee36-f433-4930-e085-eed96ae7f96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "sns.lineplot(y=total_loss['val'],x=list(range(len(total_loss['val']))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e24c99f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MwkYQm-Zs6R"
      },
      "source": [
        "torch.save(model,'/content/gdrive/My Drive/0.95_100.pt')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsTU60NVZs3U"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1pMGbFlZs0a"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}