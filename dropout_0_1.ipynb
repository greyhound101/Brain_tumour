{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dropout_0.1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHCWDo5+JhdA+yYEbrMHOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/Brain_tumour/blob/master/dropout_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hircqex3xTGT",
        "outputId": "422f803c-a7eb-4cf7-c5f8-cc4f4d3bed41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLXPjLDL0L5E",
        "outputId": "aaf3dc8f-6722-4f5e-b361-69ca4621a9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 1.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdCUrKuFAqIU",
        "outputId": "4e09dd6b-e823-4597-ead1-cdfaec273251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 29kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl\u001b[0m\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 25.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (50.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2wW7Lzjw4oJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.models as M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxtHz0Pjw8OJ"
      },
      "source": [
        "\n",
        "\n",
        "def seed_everything(seed=2020):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything(42)\n",
        "ROOT = \"/content/gdrive/My Drive/lungs\"\n",
        "BATCH_SIZE=128\n",
        "a = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\n",
        "a['Patient']=a['Patient_Week'].apply(lambda x:x.split('_')[0])\n",
        "tr = pd.read_csv(f\"{ROOT}/train.csv\")\n",
        "\n",
        "tr = pd.read_csv(f\"{ROOT}/train.csv\")\n",
        "tr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
        "chunk = pd.read_csv(f\"{ROOT}/test.csv\")\n",
        "\n",
        "\n",
        "sub = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\n",
        "sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\n",
        "sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
        "sub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\n",
        "sub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")\n",
        "\n",
        "tr['WHERE'] = 'train'\n",
        "chunk['WHERE'] = 'val'\n",
        "sub['WHERE'] = 'test'\n",
        "total = tr.append([chunk, sub])\n",
        "\n",
        "print(tr.shape, chunk.shape, sub.shape, total.shape)\n",
        "print(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n",
        "      total.Patient.nunique())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tSzLumvw8LG"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxMvh616w8Hv"
      },
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "i=0\n",
        "for patient,sudo in tqdm(total.groupby(['Patient'])):\n",
        "    for limit in sudo['Weeks'].unique():\n",
        "        data=sudo.loc[sudo['Weeks']>=limit]\n",
        "        data['difference']=data['Weeks']-limit\n",
        "        data['min_week'] = data['Weeks']\n",
        "        data.loc[data.WHERE=='test','min_week'] = np.nan\n",
        "        data['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n",
        "        base = data.loc[data.Weeks == data.min_week]\n",
        "        base = base[['Patient','FVC']].copy()\n",
        "        base.columns = ['Patient','min_FVC']\n",
        "        base['nb'] = 1\n",
        "        base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
        "        base = base[base.nb==1]\n",
        "        base.drop('nb', axis=1, inplace=True)\n",
        "        data = data.merge(base, on='Patient', how='left')\n",
        "        data['base_week'] = data['Weeks'] - data['min_week']\n",
        "        COLS = ['Sex','SmokingStatus'] #,'Age'\n",
        "        FE = []\n",
        "        data['age'] = (data['Age'] - data['Age'].min() ) \n",
        "        data['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \n",
        "        data['week'] = (data['base_week'] - data['base_week'].min() ) \n",
        "        data['percent'] = (data['Percent'] - data['Percent'].min() ) \n",
        "        FE += ['age','percent','week','BASE']\n",
        "        if sum(data[FE].isna().sum().values) >0 and not all(data['WHERE']=='test'):\n",
        "            print(limit,patient,data[FE].isna().sum().values)\n",
        "        i+=1\n",
        "        if i==1:\n",
        "            augmented=data\n",
        "        else:\n",
        "            augmented=pd.concat([augmented,data],0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsxi0V2kw8E9"
      },
      "source": [
        "augmented['i']=list(range(augmented.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB_FdGkLw8Bz"
      },
      "source": [
        "FE=['Male','Female','Ex-smoker','Never smoked','Currently smokes','difference']\n",
        "FE += ['age','percent','week','BASE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HFnDlAw7-4"
      },
      "source": [
        "for col in COLS:\n",
        "    for mod in augmented[col].unique():\n",
        "        FE.append(mod)\n",
        "        augmented[mod] = (augmented[col] == mod).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ztN_IPw78A"
      },
      "source": [
        "augmented['Patient_Week']=augmented['Patient'].astype(str)+'_'+augmented['Weeks'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1nkM6I9w75a"
      },
      "source": [
        "sub1 = augmented.loc[augmented.WHERE=='test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhRXvFYew72T"
      },
      "source": [
        "augmented['age'] = (augmented['Age']  ) / ( augmented['Age'].max() - augmented['Age'].min() )\n",
        "augmented['BASE'] = (augmented['min_FVC']  ) / ( augmented['min_FVC'].max() - augmented['min_FVC'].min() )\n",
        "augmented['week'] = (augmented['base_week']  ) / ( augmented['base_week'].max() - augmented['base_week'].min() )\n",
        "augmented['difference'] = (augmented['difference']  ) / ( augmented['difference'].max() - augmented['difference'].min() )\n",
        "augmented['percent'] = (augmented['Percent']  ) / ( augmented['Percent'].max() - augmented['Percent'].min() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfqhF0Pw70a"
      },
      "source": [
        "\n",
        "\n",
        "tr = augmented.loc[augmented.WHERE=='train']\n",
        "chunk = augmented.loc[augmented.WHERE=='val']\n",
        "sub = augmented.loc[augmented.WHERE=='test']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SawWq8Kpw7xp"
      },
      "source": [
        "for en,(i,data) in tqdm(enumerate(sub1.groupby(['Patient_Week']))):\n",
        "    sm=sum(data['difference'])\n",
        "    data['difference']=data['difference']/sm\n",
        "    data=data.sort_values(by='difference')\n",
        "    data['difference']=data['difference'].values[::-1]\n",
        "    if en==0:\n",
        "        total=data\n",
        "    else:\n",
        "        total=pd.concat([total,data],0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUmMYPHw7ua"
      },
      "source": [
        "import gc\n",
        "del sub1\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcrVlXi9w7rR"
      },
      "source": [
        "tr.shape, chunk.shape, sub.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXlGDu41w7oq"
      },
      "source": [
        "\n",
        "\n",
        "print(sub.shape)\n",
        "print(total.shape)\n",
        "sub=sub.dropna(subset=FE)\n",
        "total=total.dropna(subset=['week','BASE'])\n",
        "print(sub.shape)\n",
        "print(total.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Azts5Uw2q4b"
      },
      "source": [
        "sub['difference'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcKrtydHw7l0"
      },
      "source": [
        "\n",
        "from tensorflow.keras.regularizers import *\n",
        "C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n",
        "#=============================#\n",
        "def score(y_true, y_pred):\n",
        "    tf.dtypes.cast(y_true, tf.float32)\n",
        "    tf.dtypes.cast(y_pred, tf.float32)\n",
        "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
        "    fvc_pred = y_pred[:, 1]\n",
        "    \n",
        "    #sigma_clip = sigma + C1\n",
        "    sigma_clip = tf.maximum(sigma, C1)\n",
        "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
        "    delta = tf.minimum(delta, C2)\n",
        "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
        "    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n",
        "    return K.mean(metric)\n",
        "#============================#\n",
        "def qloss(y_true, y_pred):\n",
        "    # Pinball loss for multiple quantiles\n",
        "    qs = [0.2, 0.50, 0.8]\n",
        "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
        "    e = y_true - y_pred\n",
        "    v = tf.maximum(q*e, (q-1)*e)\n",
        "    return K.mean(v)\n",
        "#=============================#\n",
        "def mloss(_lambda):\n",
        "    def loss(y_true, y_pred):\n",
        "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
        "    return loss\n",
        "#=================\n",
        "def make_model(nh):\n",
        "    z = L.Input((nh,), name=\"Patient\")\n",
        "    emb = L.Input((1,))\n",
        "    emb1 = L.Embedding(input_dim=147,output_dim=512,embeddings_regularizer=l2(1e-2))(emb)\n",
        "    c = L.Concatenate()([z,L.Flatten()(emb1)])\n",
        "    x = L.Dense(100, activation=\"relu\", name=\"d1\")(c)\n",
        "    x = L.Dropout(0.1)(x)\n",
        "    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n",
        "    #x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n",
        "    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n",
        "    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n",
        "    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
        "                     name=\"preds\")([p1, p2])\n",
        "    \n",
        "    model = M.Model([z,emb], preds, name=\"CNN\")\n",
        "    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n",
        "    model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n",
        "    return model\n",
        "\n",
        "y = tr['FVC'].values\n",
        "z = tr[FE].values\n",
        "ze = sub[FE].values\n",
        "nh = z.shape[1]\n",
        "pe = np.zeros((ze.shape[0], 3))\n",
        "pred = np.zeros((z.shape[0], 3))\n",
        "\n",
        "net = make_model(nh)\n",
        "print(net.summary())\n",
        "print(net.count_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2hIatd_xZM-"
      },
      "source": [
        "NFOLD = 5\n",
        "kf = KFold(n_splits=NFOLD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESuINQk-WmU"
      },
      "source": [
        "tr[FE].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkgU6TS5-eQE"
      },
      "source": [
        "tr=tr.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O58F9J3oxZJ4"
      },
      "source": [
        "%%time\n",
        "cnt = 0\n",
        "EPOCHS = 800\n",
        "for tr_idx, val_idx in kf.split(z):\n",
        "    cnt += 1\n",
        "    print(f\"FOLD {cnt}\")\n",
        "    net = make_model(nh)\n",
        "    hist=net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
        "            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "    plt.show()\n",
        "    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n",
        "    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n",
        "    print(\"predict val...\")\n",
        "    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n",
        "    print(\"predict test...\")\n",
        "    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) / NFOLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR4_dLE2xZHI"
      },
      "source": [
        "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
        "unc = pred[:,2] - pred[:, 0]\n",
        "sigma_mean = np.mean(unc)\n",
        "print(sigma_opt, sigma_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1iIv4L8xZEG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_legUCjxZBB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fjKGXKlxY-Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l3oL85ixY7P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7En9aZNxY4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME3MTpMAxY1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}